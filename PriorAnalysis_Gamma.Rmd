---
title: "Gamma priors on cutpoints in an ordinal logistic model"
author: "Susannah Tysor"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE)
```

```{r packages}
library(dplyr)
library(ggplot2)
library(bayesplot)
library(rstan)
library(cowplot)
library(testthat)
library(parallel)
library(purrr)
```


```{r depends}
source('priorAnalysisGammaFunctions.R')
```

```{r options}
rstan_options(auto_write=TRUE)
```

## Prior choice

In my original phenology model, I use

* a gamma distribution for the prior on the cutpoints
* an exponential distribution distribution for the transition speed/slope $\beta$
    * normal distributions for group effects on the slope

Michael Betancourt thinks that an induced dirichlet prior may be a [good choice](https://betanalpha.github.io/assets/case_studies/ordinal_regression.html) for ordinal logistic models.

Normally, priors are bottom up - you choose a distribution for each prior. This is tricky for the cutpoints in an ordinal logistic model because they're defined on an abstract latent space so you can't easily use your domain expertise, but a good prior is incredibly important in these models, because ordered logistic models with covariates are inherently non-identifiable. (Because $beta$s and cutpoints depend on one another.)

Betancourt says that
> To avoid the non-identifiability of the interior cut points from propagating to the posterior distribution we need a principled prior model that can exploit domain expertise to consistently regularize all of the internal cut points at the same time. Regularization of the cut points, however, is subtle given their ordering constraint. Moreover domain expertise is awkward to apply on the abstract latent space where the internal cut points are defined.

# Goals
1) Understand the dynamics of the gamma and induced dirichlet priors and 2) compare them

# Simulate data
First let's simulate some data that's kind of like mine.

We have three potential states `K` and a latent effect/covariate `x`. Data is most likely to be collected around state 2. `x` is always positive.

I'll simulate 2 datasets - one with a fast transition speed ($\beta = 2$) and one with a slow transition speed ($\beta = 0.5$). I want the transitions to occur at the same `x` for both datasets. The halfway transition points `h` will be at `x= 8` and `x=12`. So the cutpoints `c` for the slow transition will be at 4 and 6 and for the fast transition at 16 and 24.

Here is what a slow (beta=0.5) and fast (beta=2) transition look like. My data is more like the fast transition - a phenological period with transitions of beta=0.5 would be occuring over a really long time period.
```{r transitions}

logistic2 <- function(x, beta, c) {
  y <- 1/(1+exp(-beta*x + c))
}

x <- c(0:20)
logisticexp <- data.frame(x, slow = logistic2(x, 0.5, 6), fast=logistic2(x, 2, 24), medium=logistic2(x, 1,12)) %>%
  tidyr::pivot_longer(cols=c(slow, medium, fast))

ggplot(logisticexp, aes(x=x, y=value, color=name)) +
  geom_line()
```

## Simulate datasets

```{r set simulation parameters}
N <- 500
K <- 3

beta <- data.frame(transition = c("slow", "medium", "fast"), beta=c(0.5, 1, 2))
cutpoints <- data.frame(c.1= c(4,8,16), c.2=c(6, 12, 24), transition=c("slow", "medium", "fast"))

simu_pars <- merge(beta, cutpoints)

# half transition points, engineered to be identical all transitions
h1 <- unique(simu_pars$c.1/simu_pars$beta )
h2 <- unique(simu_pars$c.2/simu_pars$beta )
h <- c(h1, h2)

# covariate over full range of heat accumulation (risto scale) Jan-Julyish
#x <- rtnorm(n=N, mean=mean(h), sd=2, min=0, max=20) #covariate
x <- runif(n=N, min=0, max=20)

testthat::test_that("half transition points identical", {
  testthat::expect_equal(length(h), 2)
})
```

```{r simulateData}

inputs_for_sim <- split(simu_pars, simu_pars$transition) %>%
  map(.f=function(y) {list("N" = N, "K" = K, "c" = c(y$c.1, y$c.2), "beta"=y$beta, "h" = h, "x" = x)})

simdat <- map(inputs_for_sim, simulate_data, groups=FALSE)

# plot simulated data
simdf <- purrr::map(simdat, .f = function(x) {x[c("x", "y")]}) %>%
  purrr::map_dfr(.f = bind_rows, .id=".id") 

p1 <- ggplot(simdf, aes(x=x, y=y)) +
  geom_jitter(shape=1, height=0.1, alpha=0.5) +
  geom_vline(xintercept = h) +
  ggtitle("Simulated data with cutpoints") +
  facet_grid(.id ~ .)

p2 <- ggplot(simdf, aes(x=x, colour=as.factor(y))) +
  stat_ecdf() +
  geom_vline(xintercept=h) +
  theme(legend.position = "none") +
  ggtitle("Cumulative x for 3 states") +
  facet_grid(.id ~ .)

cowplot::plot_grid(p1, p2, ncol=2)

```

## Recapture parameters: Gamma

Now I want to try to recapture parameters.

I need to know how good I have to be at choosing the shape parameter for the gamma prior on the cutpoints to recapture parameters as well. So I'll choose a prior for gamma that roughly centers it between the cutpoints, which I think is a "good" option (though given the long right tail, maybe a lower value would be better). Then halve and double it.

I also want to understand how the exponential prior on beta affects the ability to recapture parameters. I'll try rates of 1,2,3.


### Gamma prior with covariate
```{r paramsForGammaModels}

beta_rate <- c(1:3) # rate parameters for exponential prior on beta

# shape and rate parameters for gamma prior on cutpoints, all centered on mean transition point

# make shape and rate parameters for a gamma distribution centered on the mean of h (a vector) with spread scaled by a factor (scaler). Larger factors make the distribution skinnier and smaller ones make it fatter. "h" are the half transition points, so the mean is basically the midpoint of your state 2 in a 3 state system when you have data like mine.
make_gamma_pars <- function(factor, h) {
  center <- mean(h)
  shape <- mean(h) * factor
  cut_rate <- shape/center
  sr <- data.frame(shape, cut_rate)
  return(sr)
}

factors <- list(fat=0.25, normal=1, skinny=2)

gamma_pars <- purrr::map(factors, make_gamma_pars, h=h)  
gamma_parsdf <-purrr::map_dfr(gamma_pars, .f=bind_cols, .id=".id") 

# format for plotting
sr <- gamma_pars %>%
  map(function(x) {rgamma(1000, x$shape, x$cut_rate)}) %>%
  bind_cols() %>%
  tidyr::pivot_longer(cols=names(factors))

# plot gamma priors
ggplot(sr, aes(x=value, fill=name)) +
  geom_density() +
  facet_grid(name ~ .) +
  ggtitle("Gamma priors all centered on 10", subtitle = "with different shape and rate params")


# make a nice dataframe with all combinations params used to simulate data and model params used to try to recover those params
parframe_gam <- merge(gamma_parsdf, beta_rate) %>%
  dplyr::rename(gammaid=.id, beta_rate=y) %>%
  merge(simu_pars)
parframe_gam$h1 <- h[1]
parframe_gam$h2 <- h[2]
parframe_gam$modelid <- 1:nrow(parframe_gam) #label the models


knitr::kable(parframe_gam, caption="model configurations used to try to recapture params")
```

So there are 3 simulated datasets - 1 for each of three transition speeds. I'm going to try to fit them both with 3 rates for the beta's exponential prior and 3 shape and rate combinations for the cutpoints' gamma prior, a total of 27 model runs. 

As beta gets larger, cutpoints get smaller and cutpoints priors need to be able to "reach" small values. As beta gets smaller, cutpoints need to be able to "reach" larger values. 

```{r GammaRecapture, include=FALSE, cache=TRUE}
# Parallel code actually executes in parallel when running this chuck as a chunck with ctrl+shift+enter. It does not if you run it line by line. I don't know why. 

# format parframe so it works with parLapply better
parlist_gam <- split(parframe_gam, seq(nrow(parframe_gam))) 
names(parlist_gam) <- parframe_gam$modelid


# run all models, parallelized

# make a cluster using half your cores
no_cores <- parallel::detectCores()/2
cl <- parallel::makeCluster(no_cores)

# export the stuff you need to run on the cluster
parallel::clusterExport(cl, c("fit_gamma_model", "parlist_gam", "simdat"))
parallel::clusterEvalQ(cl, c(library(rstan), library(StanHeaders)))

# fit the models
gammafits <- parallel::parLapply(cl, parlist_gam, function(x) {fit_gamma_model(simdatlist = simdat, pars=x, groups=FALSE)})

parallel::stopCluster(cl) #close the cluster

```

```{r gammaExtract}
# extract params
paramsgam <- lapply(gammafits,
                    function(x) {data.frame(rstan::extract(x) ) } )
#param summary
sum_gam <- lapply(paramsgam, summary)

#bind true and model pars even tho it will make a giant df

paramsgam <- map(paramsgam, label_names, label="model")
parlist_gam <- map(parlist_gam, label_names, label="true")

paramsgam <- map2(paramsgam, parlist_gam, cbind)
```

```{r gammaParPlots, fig.height=2}
#plot params and diffs
#map(paramsgam, parplot)
map(paramsgam, posterior_differencer) %>%
    map2(.y=paramsgam, .f=diffplotter)
```

```{r GammaHDPI}
# calculate whether true value is in HPDI


in50 <- map(paramsgam, calc_HPDI, prob=0.5)

in75 <- map(paramsgam, calc_HPDI, prob=0.75)

in90 <- map(paramsgam, calc_HPDI, prob=0.90)

# recaptured parameters
perform50 <- map_df(in50, bind_rows, .id="modelid") %>%
  dplyr::mutate(modelid=as.integer(modelid)) %>%
  full_join(parframe_gam) 
perform90 <- map_df(in90, bind_rows, .id = "modelid") %>%
  dplyr::mutate(modelid=as.integer(modelid)) %>%
  full_join(parframe_gam)

# proportion recaptured parameters
prop_recaptured50 <- perform50 %>%
  group_by(modelid) %>%
  summarise(all_captured = sum(inint)/n()) %>%
  full_join(parframe_gam) %>%
  arrange(beta, desc(all_captured))

prop_recaptured90 <- perform90 %>%
  group_by(modelid) %>%
  summarise(all_captured = sum(inint)/n()) %>%
  full_join(parframe_gam) %>%
  arrange(beta, desc(all_captured))

#knitr::kable(prop_recaptured50, caption = "Proportion of params recaptured in 50% HPDI")
#knitr::kable(prop_recaptured90, caption = "Proportion of params recaptured in 90% HPDI")
```

```{r plotGammaRecaptureoverall}
ggplot(prop_recaptured50, aes(x=as.factor(shape), y=as.factor(beta_rate), fill=all_captured)) + 
  geom_tile(colour="white") +
  geom_text(aes(label=paste(cut_rate, beta_rate, sep=","))) +
  facet_wrap("transition", scales="free") +
  ggtitle("Proportion of parameters recaptured in 50% HPDI") +
  scale_fill_gradient(low="white", high="steelblue")

ggplot(prop_recaptured90, aes(x=gammaid, y=as.factor(beta_rate), fill=all_captured)) + 
  geom_tile(colour="white") +
  geom_text(aes(label=paste(cut_rate, beta_rate, sep=","))) +
  facet_wrap("transition", scales="free") +
  ggtitle("Proportion of parameters recaptured in 90% HPDI") +
  scale_fill_gradient(low="white", high="violetred")
```
Parameters are most likely to be recaptured when transitions are medium fast (beta=1), shape parameters are higher than cutpoints, and beta is not too constrained (low beta rate). However when transitions are fast (and cutpoints correspondingly larger, very high shape values do not work well.) Choosing shape values that center the gamma prior relatively near the second cutpoint seems to work best.

This is tricky to do, however, without knowing beta - or having a lot of knowledge of your system.

# COMPARE NORMALLY DISTRIBUTED VS UNIFORM COVAR
```{r plotGammaRecapture}
knitr::kable(perform50, caption = "Ability of the different models to recapture parameters in 50% HPDI. Each model only run only once, don't yell at me.")

ggplot(perform50, aes(x=params, y=as.factor(modelid), fill=inint)) + 
  geom_tile(color="black") +
  scale_fill_viridis_d(begin=0.1, end=0.9)  +
  geom_text(aes(label=paste(shape, beta_rate, sep=","))) +
  facet_wrap("beta", scales="free") +
  ggtitle("Parameters recaptured in 50% HPDI", subtitle = "faceted by beta")
    


knitr::kable(perform90, caption = "Ability of the different models to recapture parameters in 90% HPDI. Each model only run only once, don't shoot me.")

ggplot(perform90, aes(x=params, y=as.factor(modelid), fill=inint)) + 
  geom_tile(color="black") +
  scale_fill_viridis_d()  +
  geom_text(aes(label=paste(shape, beta_rate, sep=","))) +
  facet_wrap("beta", scales="free") +
  ggtitle("Parameters recaptured in 90% HPDI", subtitle = "faceted by beta")

```



#### Slow transition
Estimates are better for slow parameters when shape parameters are large. When shape parameters are lower, beta and cutpoints are estimated more poorly, but the relationship between beta and cutpoints (the transitions h1 and h2) are still captured well.

#### Fast transition
Estimates are better for fast transitions when choosing a shape parameter near the cutpoints (20).

The proportion of parameters recaptured (in the 50% HDPI) is highest when the transitions are slow (beta=0.5) and the shape parameter for the gamma prior is low. 

When transitions are slow, large shapes for gamma make it difficult to recapture parameters. When transitions are fast, constraining beta too much (eg. beta rate = 3) harms model performance. Moderate gamma shape parameters work best. 

Getting the shape parameter "right" is more important than the beta rate parameter (over the ranges I've considered anyway). Unfortunately, "right" depends on the speed of transition, which we .... don't know in advance. Given the realities of phenology, I think that it's fair to say the speed is > 1 (at least with forcing units/x scaled the way they are.)

Sadly, slow transition models are easier to fit. 

The parameters I'm most interested in are the transition parameters (calculated as c/beta), h1 and h2. Unfortunately, these seem relatively difficult to recapture well. In the 50% HDPI, no rapid transition model gets either of these right. The slow model can get them right with a low shape and a constrained beta. In the 90% HDPI, models with rapid transition datasets *almost* catch up to the slow transitions. 

Truly unfortunately, the prior parameters (gamma shape) that make recapture possible are basically opposite for slow and fast transition data. Slow transitions require small to moderate shapes to recapture parameters, but fast transitions require moderate to high transitions to fit. Fast transitions also need a less constrained beta.

Shape = 10 was large for slow transitions and small for fast transitions and made it impossible for both models to recapture true params. 

So how might I go about choosing a shape parameter for a gamma prior? I think I would assume my transition speed (beta) must be >1 and is probably ~2 because of biology. So then, a good gamma shape param choice would probably be around the values your data starts transitioning from state 2 to 3. This feels yucky.

## Group effects
In my data I have different clusters and I want to know if and how the groups within those clusters differ. 

I'm going to start with one cluster of 7 groups (like my site variable). I'm going to model group effects as deviations from the population mean transition speed, $beta$. A very large $beta$, probably unreasonably large would be +/-0.5.

So I'll simulate 7 groups with `N=500` observations for each group. 

I'll simulate 3 groups of group effects - big, small, and mixed. Effects for the groups are pulled from a normal distribution with `mean=0` and `sd=0.0.15`. They're deviations from the population mean. 


```{r group simulation, fig.height=6}

N = 500*7
G = 7

# simulate individual group effects
gbeta_mu <- 0
gbeta_sd <- 0.05

gbeta_vec <- rnorm(G, mean=gbeta_mu, sd=gbeta_sd) # group effects
gbeta <- sort(sample(gbeta_vec, size=N, replace=TRUE)) # assign a group effect to every observation for stan (like x)
GID <- as.numeric(as.factor(gbeta)) # label the groups
groupeffects <- data.frame(gbeta, GID) %>%
  unique()

 # simu_pars_group <- merge(simu_pars, groupeffects) %>%
 #   tidyr::pivot_wider(names_from = GID, names_prefix="gbeta", values_from = gbeta)

#x <- rtnorm(n=N, mean=mean(h), sd=2, min=0, max=20) #covariate centered around transitions (ignoring groups)
x <- runif(n=N, min=0, max=20)
hist(x)

inputs_for_sim_group <- split(simu_pars_group, simu_pars$transition) %>%
  map(.f=function(y) {list("N" = N, "K" = K, "G"=G, "c" = c(y$c.1, y$c.2), "beta"=y$beta, "gbeta" = gbeta, "GID"= GID, "x" = x)})

simdat_group <- map(inputs_for_sim_group, simulate_data, groups=TRUE)

simdf <- purrr::map(simdat_group, .f=function(x) {x[c("x", "y", "GID")]}) %>%
  purrr::map_dfr(.f = bind_rows, .id=".id") 

ggplot(simdf, aes(x=x, y=y)) +
  geom_jitter(shape=1, height=0.1, alpha=0.5) +
  geom_vline(xintercept = h) +
  ggtitle("Simulated data with cutpoints") +
  facet_grid(GID ~ .id)

ggplot(simdf, aes(x=x, colour=as.factor(GID))) +
  stat_ecdf() +
  geom_vline(xintercept=h) +
  theme(legend.position = "none") +
  ggtitle("Cumulative x for 3 states") +
  facet_grid(y ~ .id)
```

## Gamma recapture 
Can a model with the gamma prior recapture the parameters used to simulate the data?

The group effect prior ~ $\mathrm{normal}(0,sigma_{group})$ with $sigma_{group} ~ \mathrm{exponential}(4)$

```{r paramsForGroup}


# prep group effects for easy adding to parameters frame
group_betas <- t(groupeffects$gbeta)
colnames(group_betas) <- paste0("betag.", 1:7)

parframe_gam <- parframe_gam %>% 
  merge(group_betas) %>% # add group effects
  # calculate group h's 
  dplyr::select(beta, starts_with("c."), starts_with("betag")) %>%
  distinct() %>%
  tidyr::pivot_longer(cols=starts_with("betag"), names_to = "group", values_to="betag") %>%
  dplyr::mutate(h1 = c.1/(beta+betag), h2=c.2/(beta+betag)) %>% # actual h calculation - the rest is inelegant formatting
  tidyr::separate(group, into = c("par", "group")) %>%
  tidyr::pivot_wider(names_from = group, values_from = c("betag", "h1", "h2"), names_sep=".") %>%
  dplyr::select(-par) %>%
  full_join(parframe_gam) %>%
  dplyr::filter(gammaid != "skinny") # drop parameter sets that don't even work in the "easy" case.
  

# make list of model sets for easier parallelization

parlist_gam <- split(parframe_gam, seq(nrow(parframe_gam))) 
names(parlist_gam) <- parframe_gam$modelid
```
```{r gammaRecaptureGroup, include=FALSE, cache=TRUE}

#fit_gamma_model(simdat_group, parlist_gam[[1]], groups=TRUE)

# run all models, parallelized

# make a cluster, leaving 20 cores free for other folks
no_cores <- parallel::detectCores() - 20
cl <- parallel::makeCluster(no_cores)

parallel::clusterExport(cl, c("fit_gamma_model", "parlist_gam", "simdat_group"))
parallel::clusterEvalQ(cl, c(library(rstan), library(StanHeaders)))

gammafits <- parallel::parLapply(cl, parlist_gam, function(x) {fit_gamma_model(simdatlist = simdat_group, pars=x, groups=TRUE)})

parallel::stopCluster(cl) #close the cluster
```

Running models with 1000 warmup, 2000 total gives no divergence warnings or anything, but does have ESS issues. Increasing the length of the model run resolves all warnings.
```{r modelChecking, warning=TRUE}

for (i in 1:length(gammafits)) {
  print(paste("model", names(gammafits)[i]))
  rstan:::throw_sampler_warnings(gammafits[[i]])
}
map(gammafits, function(x) {rstan:::throw_sampler_warnings(x)})
```
```{r gammaExtractGroup}
# extract params
paramsgam <- lapply(gammafits,
                    function(x) {data.frame(rstan::extract(x) ) } ) %>%
   map(label_names, label="model") #bind true and model pars even tho it will be huge and repetitive

#paramsgam <- map(paramsgam, label_names, label="model")
parlist_gam <- map(parlist_gam, label_names, label="true")

paramsgam <- map2(paramsgam, parlist_gam, cbind)
```
```{r gammaHPDIgroup}


calc_HPDI <- function(params, prob) {
    low <- params %>% dplyr::summarise_at(vars(ends_with("model")), HPDIlow, prob=prob)
    high <- params %>% dplyr::summarise_at(vars(ends_with("model")), HPDIhigh, prob=prob)

    # awkward formatting
    hdpis <- dplyr::full_join(low, high) %>%
        select(-contains("lp"))
    colnames(hdpis) <- stringr::str_replace(colnames(hdpis), "_model", "")

    # true param
    true <- params %>% dplyr::summarise_at(vars(ends_with("true")), unique)
    colnames(true) <- stringr::str_replace(colnames(true), "_true", "")
    true <- select(true, colnames(hdpis))

    # more awkward formatting
    compframe <- dplyr::full_join(hdpis, true) %>%
        t(.) %>%
        data.frame()
    colnames(compframe) <- c("low", "high", "true")
    compframe$params <- rownames(compframe)

    # true param in interval?
    tf <- compframe %>% mutate(inint = true > low & true < high)
    return(tf)

}

# calculate whether true value is in HPDI


in50 <- map(paramsgam, calc_HPDI, prob=0.5)

in75 <- map(paramsgam, calc_HPDI, prob=0.75)

in90 <- map(paramsgam, calc_HPDI, prob=0.90)

# recaptured parameters
perform50 <- map_df(in50, bind_rows, .id="modelid") %>%
  dplyr::mutate(modelid=as.integer(modelid)) %>%
 full_join(parframe_gam) 
perform90 <- map_df(in90, bind_rows, .id = "modelid") %>%
  dplyr::mutate(modelid=as.integer(modelid)) %>%
 full_join(parframe_gam)

# proportion recaptured parameters
prop_recaptured50 <- perform50 %>%
  group_by(modelid) %>%
  summarise(all_captured = sum(inint)/n()) %>%
  full_join(parframe_gam) %>%
  arrange(beta, desc(all_captured))

prop_recaptured90 <- perform90 %>%
  group_by(modelid) %>%
  summarise(all_captured = sum(inint)/n()) %>%
  full_join(parframe_gam) %>%
  arrange(beta, desc(all_captured))

```
```{r plotGammaRecaptureOverallGroup}
ggplot(prop_recaptured50, aes(x=as.factor(shape), y=as.factor(beta_rate), fill=all_captured)) + 
  geom_tile(colour="white") +
  geom_text(aes(label=paste(cut_rate, beta_rate, sep=","))) +
  facet_wrap("transition", scales="free") +
  ggtitle("Proportion of parameters recaptured in 50% HPDI", subtitle = "faceted by beta") +
  scale_fill_gradient(low="white", high="steelblue")

ggplot(prop_recaptured90, aes(x=gammaid, y=as.factor(beta_rate), fill=all_captured)) + 
  geom_tile(colour="white") +
  geom_text(aes(label=paste(cut_rate, beta_rate, sep=","))) +
  facet_wrap("transition", scales="free") +
  ggtitle("Proportion of parameters recaptured in 90% HPDI") +
  scale_fill_gradient(low="white", high="violetred")
```

```{r plotGammaRecaptureGroup}
knitr::kable(perform50, caption = "Ability of the different models to recapture parameters in 50% HPDI. Each model only run only once, don't yell at me.")

ggplot(perform50, aes(x=params, y=as.factor(modelid), fill=inint)) + 
  geom_tile(color="black") +
  scale_fill_viridis_d(begin=0.1, end=0.9)  +
  geom_text(aes(label=paste(shape, beta_rate, sep=","))) +
  facet_wrap("beta", scales="free") +
  ggtitle("Parameters recaptured in 50% HPDI", subtitle = "faceted by beta")
    


knitr::kable(perform90, caption = "Ability of the different models to recapture parameters in 90% HPDI. Each model only run only once, don't shoot me.")

ggplot(perform90, aes(x=params, y=as.factor(modelid), fill=inint)) + 
  geom_tile(color="black") +
  scale_fill_viridis_d()  +
  geom_text(aes(label=paste(shape, beta_rate, sep=","))) +
  facet_wrap("beta", scales="free") +
  ggtitle("Parameters recaptured in 90% HPDI", subtitle = "faceted by beta")

```

The only parameters *ever* recaptured are cutpoints. And those only rarely.

## Possible fixes?
- add more clusters
- add more data
- pick larger effects

This is a very serious kind of failure because it looks like the model fits, but none of the parameters are recaptured.
