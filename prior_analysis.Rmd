---
title: "Choosing priors in an ordinal logistic model"
author: "Susannah Tysor"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  html_document:
    theme: flatly
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message=FALSE)
```

```{r packages}
library(dplyr)
library(ggplot2)
library(bayesplot)
library(rstan)
library(cowplot)
library(testthat)
library(parallel)
library(purrr)
library(tidyr)
library(rethinking)
```

```{r depends}
source('prior_analysis_functions.R')
```

```{r options}
rstan_options(auto_write=TRUE)
```

```{r global}
# each version of a model should be run how many times?
reps <- 10
```


Choosing priors for cutpoints in an ordinal logistic model is tricky. Michael Betancourt developed [an indirect prior](https://betanalpha.github.io/assets/case_studies/ordinal_regression.html) on the cut points that may be a good choice:

> Applying the Dirichlet prior model to the full ordinal model doesnâ€™t regularize the internal cut points directly, but it does regularize them indirectly through its regularization of the ordinal probabilities.


I didn't know about this option when I built an ordinal logistic model, and I want to compare it to what I actually used. When I modelled lodgepole pine flowering phenology as a function of forcing units, I used

* a gamma distribution for the prior on the cut points, $c$
* an exponential distribution distribution for the transition speed/slope, $\beta$

I chose a gamma prior on the cutpoints because it was continuous and always positive and I could center it where I wanted relatively easily. I struggled to get the model to fit or provide consistent estimates, especially after adding groups.

## Choosing priors
Normally, priors are bottom up - you choose a prior distribution for each parameter in your model. This is tricky for the cut points in an ordinal logistic model because they're defined on an abstract latent space where your domain expertise is hard to apply. But you really want to choose a good prior in ordinal logistic models, because ordinal logistic models with covariates are inherently non-identifiable. (Because $\beta$ and $c$ depend on one another.) As Betancourt says -

> To avoid the non-identifiability of the interior cut points from propagating to the posterior distribution we need a principled prior model that can exploit domain expertise to consistently regularize all of the internal cut points at the same time. Regularization of the cut points, however, is subtle given their ordering constraint. Moreover domain expertise is awkward to apply on the abstract latent space where the internal cut points are defined. ([Betancourt](https://betanalpha.github.io/assets/case_studies/ordinal_regression.html))

The induced ordinal logistic prior on the cutpoints is a sort of top-down prior that works through the ordinal probabilities.

If I simulate data like mine using an ordinal logistic model, can I recapture the parameters using an ordinal logistic model with a gamma or induced dirichlet prior on the cut points?

Recapturing $\beta$ and $c$ well isn't as important as recapturing the transition points, $h$. $h$ are the points at which half of the population has transitioned from one state to another and is calculated as $h = \frac{c}{\beta}$. 

Eventually I will want to consider how different groups affect $h$ *e.g* $h = \frac{c + \alpha_{site_i} + \alpha_provenance_i}{\beta}$). Estimating $\beta$, $c$, and $\alpha$ well will probably be more important then. But for now, let's just consider an ordinal logistic model with a covariate/latent effect and no groups.

# Goals

1) When can I recapture parameters with a gamma prior?
2) When can I recapture parameters with an induced dirichlet prior?
3) Which one works best?

# Simulate data

First let's simulate some data that's kind of like mine.

I have three potential states $K$ and a covariate $x$. $x$ is always positive and measurements are only made when $x$ is between about $8$ and $18$. Data collection begins when workers notice trees are approaching or entering state $2$ and stops as trees enter state $3$.

I simulate 3 datasets with 

* slow ($\beta = 0.5$), 
* medium ($\beta = 1$), and 
* fast ($\beta = 2$) transition speeds. 

I want the transitions to occur at the same $x$ for both datasets. The transition points $h$ will be at `x= 10` and `x=15`. $h$ is calculated with cut points $c$ and transition speed $\beta$, that is $h = \frac{c}{\beta}$ So while the transition points will stay the same in each simulated dataset, the cut points will differ. 

Cut points for the slow transition dataset will be at 
* 5 and 7.5, at 
* 10 and 15 for the medium and at 
* 20 and 30 for the fast transition.

Here is what the first transition between states looks like for those transitions. My data is more like the fast transition - a phenological period with transitions of beta=0.5 would be occurring over an unrealistically long time period (like January to June instead of a 2 week period in late spring.

```{r transitions, fig.height=3, fig.width=4}

logistic2 <- function(x, beta, c) {
  y <- 1/(1+exp(-beta*x + c))
}

x <- c(0:20)
logisticexp <- data.frame(x, slow = logistic2(x, 0.5, 5), fast=logistic2(x, 2, 20), medium=logistic2(x, 1,10)) %>%
  tidyr::pivot_longer(cols=c(slow, medium, fast))

ggplot(logisticexp, aes(x=x, y=value, color=name)) +
  geom_line()
```

I simulated data with the Stan program

```{r}
writeLines(readLines("simulate/covar_sim.stan"))
```

```{r simulate_data}
simulation_input <- set_simulation_parameters(N=500)

simdat <- purrr::map(.x = simulation_input$inputlist, .f = simulate_data, groups=FALSE)

plot_simulated_data(simdat, simulation_input$inputlist)

```

# Recapture parameters with gamma

Now I want to try to recapture parameters.

I need to know how good I have to be at choosing the shape and rate parameters for the gamma prior on the cut points to recapture the "true" slope and cut points parameters as well. (The mean of the gamma is the shape divided by the rate.)  

Choosing where to center the gamma isn't obvious since cut points are on that "abstract latent space." Since $c = \beta h$, though, we have some idea of the range of possible values. 

If we center a gamma distribution around state $2$, how much wiggle room does it need to find the cut points? When $\beta=0.5$, $12.5$ is $6.25$ on the cut point space and when $\beta=2$, $12.5$ is $25$ on the abstract latent space. The larger $\beta$ is, the bigger and further apart the cut points are. The smaller $\beta$ is, the smaller and closer together the cut points are. Since we don't know $\beta$ in advance, we probably have to choose a fat gamma so we can access the full range of values where cut points could be. On the other hand, we might end up estimating combinations of $c$ and $\beta$ that get $h$ right, but both $c$ and $\beta$ very wrong. 

(The more categories you have, the harder this will be).

I also want to understand how the exponential prior on $\beta$ affects the ability of the model to recapture parameters. I'll try rates ($\lambda$) of 1,2,3.


```{r paramsForGammaModels}

beta_rate <- c(1:3) # rate parameters for exponential prior on beta

# shape and rate parameters for gamma prior on cut points, all centered on mean transition point

factors <- list(fat=0.10, middling=0.25, skinny=1)

pars_gamma <- purrr::map(factors, make_gamma_pars, h=simulation_input$h)  

#plot gamma priors
pars_gamma %>%
  # format for plotting
  purrr::map(function(x) {rgamma(1000, x$shape, x$cut_rate)}) %>%
  bind_cols() %>%
  tidyr::pivot_longer(cols = names(factors)) %>%
  # plot
  ggplot(aes(x=value, fill=name)) +
  geom_density(alpha=0.5) +
 # facet_grid(name ~ .) +
  ggtitle("Gamma priors all centered on 10", subtitle = "with different shape and rate params")

# make a nice dataframe with all combinations params used to simulate data and model params used to try to recover those params
parframe_gam <- pars_gamma %>%
  purrr::map_dfr(.f=bind_cols, .id=".id") %>%
  merge(beta_rate) %>%
  dplyr::rename(gammaid=.id, beta_rate=y) %>%
  merge(simulation_input$pars)
parframe_gam$h1 <- simulation_input$h[1]
parframe_gam$h2 <- simulation_input$h[2]
parframe_gam$modelid <- 1:nrow(parframe_gam) #label the models

# format parframe so it works with parLapply better
parlist_gam <- make_parframe_list(parframe_gam)

# model parameterizations
parameterizations_gam <- select(parframe_gam, gammaid, shape, cut_rate, beta_rate) %>%
  distinct() %>%
  arrange(gammaid, beta_rate)

table_gam <- parframe_gam %>% 
  select(transition, modelid, gammaid, beta_rate) %>%
  group_by(transition, modelid, gammaid, beta_rate) %>%
  tidyr::pivot_wider(names_from=transition, values_from=modelid) %>%
  tidyr::unite("modelids", c("fast", "medium", "slow"), sep=", ", remove=TRUE) %>%
  right_join(parameterizations_gam) %>%
  select(modelids, gammaid, shape, cut_rate, beta_rate)

```

So there are three simulated datasets - one for each of three transition speeds. I'm going to try to fit them both with three rates for the $\beta$'s exponential prior and three shape and rate combinations for the cut points' gamma prior, a total of 27 models. I'm going to do `r reps` runs of each model in Stan.

```{r paramsTableGamma}
knitr::kable(table_gam, caption="Prior parameters - gamma prior models. modelids are unique identifiers for each combination of prior parameters and simulated datasets, in order from fast transition to slow transition dataset")
```


```{r GammaRecapture, include=FALSE, warning=TRUE, }
# # Parallel code actually executes in parallel when running this chuck as a chunk with ctrl+shift+enter. It does not if you run it line by line. I don't know why. 
# 
# # run all models, parallelized
# 
# # make a cluster using half your cores
# no_cores <- parallel::detectCores()/2
# cl <- parallel::makeCluster(no_cores)
# 
# # export the stuff you need to run on the cluster
# parallel::clusterExport(cl, c("fit_gamma_model", "parlist_gam", "simdat"))
# parallel::clusterEvalQ(cl, c(library(rstan), library(StanHeaders)))
# 
# 
# for (i in 1:reps) {
#   fits_gam <- parallel::parLapply(cl, parlist_gam, function(x) {fit_gamma_model(simdatlist = simdat, pars=x, groups=FALSE)})
#  # fits_gam_list[[i]] <- fits_gam
#  saveRDS(fits_gam, file = paste0("gamma/covar_runs/run", i, ".rds"))
#  rm(fits_gam) # how much ram do you think you have? Don't crash everything.
#  gc() # clean your room 
# }
# 
# parallel::stopCluster(cl) #close the cluster

```

The Stan program implementing these models is

```{r}
writeLines(readLines("gamma/gamma_covar.stan"))
```

```{r readGammaModels, warning=FALSE, cache=TRUE}

# pull in parameters and info on divergences, etc from saved stanfit objects

extracts_gam <- extract_pars_and_problems(path="gamma/covar_runs/", parlist=parlist_gam, reps=10)
```

## Model check

Here's a very cursory check for problems after fitting the models - divergences and issues with rhats and neff_ratios.

```{r gammaModelCheck}
problems_gam <- extracts_gam$problems %>%
  purrr::map_dfr(bind_rows, .id=".id") %>%
  rename(run=.id) %>%
  group_by(modelid) %>%
  summarize(bad_proportion = n()/reps, bad_count = n(), divergences_mean=mean(divergences), bad_rhats_mean=mean(bad_rhats), bad_neff_mean=mean(bad_neff))
  
knitr::kable(problems_gam, caption = "Models with divergences, rhats > 1, or neff_ratios < 0.1 - gamma")

```

`r nrow(problems_gam)` out of 27 models had problems with Rhat or neff_ratio in some number of the `r reps` model runs. That's not ideal. No divergences ever though. 


```{r gammaPrettyModels}
noproblem_models_gam <- dplyr::filter(parframe_gam, !modelid %in% problems_gam$modelid) %>% arrange( transition, gammaid)
knitr::kable(noproblem_models_gam, caption="model parameterization-dataset combinations that never had a (very obvious) problem")
```

Models with slow transition speeds were less likely to have problems. Slow transitions with "loose" priors on $\beta$ run into trouble fitting- except when the cutpoints are highly constrained near the real values by a skinny gamma.

```{r}

diagnosis_framer <- function(badframe, goodframe) {
  bad <- select(badframe, modelid) %>%
    mutate(modeldiag = "poor", modelid = as.integer(modelid))
  
  good <- select(goodframe, modelid) %>%
    mutate(modeldiag = "good")
  
  diagnosis <- full_join(bad, good)
  
  return(diagnosis)
}

diag_gam <- diagnosis_framer(problems_gam, noproblem_models_gam)
```

```{r gammaExtract}

# extract parameters from stan objects and pair with true param values
#params_gam <- bind_true_model_pars(fits_gam, parlist_gam)
params_gam <- extracts_gam$pars %>%
  purrr::map_dfr(bind_rows, .id = ".id") %>%
  rename(run=.id) %>%
  left_join(diag_gam, by=c("modelid_true" = "modelid"))

```

## Parameter estimates

Parameter estimates for each run of each model are plotted below. 

As you can see, not all runs of the same model on the same data produce the same results.


```{r plotGammaParams, fig.cap="Plots of parameter estimates from models with different values for the prior. Dotted lines show models with poor model diagnostics - Rhat too big, Neff too small, etc."}

plot_densities_betaxgamma <- function(modelpar, truepar) {
  plot <- ggplot(params_gam, aes(x=get(modelpar), group=interaction(run, modelid_true), fill=transition_true, colour=transition_true, linetype=modeldiag)) +
    geom_density(alpha=0.1) +
    geom_vline(aes(xintercept=get(truepar), colour = transition_true), alpha=1) +
    facet_grid(gammaid_true ~ beta_rate_true) +
    ggtitle(strsplit(modelpar, split = "_model"), subtitle = "facet rows are gamma prior param, cols are beta prior param")
  print(plot)
}

plot_densities_betaxgamma("c.1_model", "c.1_true")
plot_densities_betaxgamma("c.2_model", "c.2_true")
plot_densities_betaxgamma("beta_model", "beta_true")

plot_densities_betaxgamma("h1_model", "h1_true")
plot_densities_betaxgamma("h2_model", "h2_true")

```
Some model runs do not recapture parameters, even when (an admittedly naive consideration of) diagnostics are good. In some cases, it looks like the model is exploring two "sides" of the parameter space and missing the middle  (See cutpoint 1 graph slow transition - fat gamma, beta rate=1)

While fast transition datasets lead to wider estimates of $\beta$ and $c$, they have narrower estimates of $h$. 
## Recapture rate

Of the 5 parameters ($\beta$, $c$ and derived $h$), how many are recaptured by the model?

```{r GammaHDPI, message=FALSE, warning=FALSE, cache=TRUE}
# calculate whether true values are in HPDI

# recaptured_gam <- which_params_recaptured(params_gam) # in or out

recaptured_gam <- params_gam %>% # are parameter estimates in the 50 or 90% HPDI?
  select(-modeldiag) %>%
  split(.$modelid) %>%
  which_params_recaptured()
  
prop_recaptured_gam <- calc_num_recaptured_overall(recaptured_gam, parframe_gam) # average number of parameters recaptured by each model
prop_recaptured_by_param_gam <- calc_recaptured_by_param(recaptured_gam, parframe_gam) %>% #proportion of model runs where a parameter is returned correctly
  purrr::map_dfr(bind_rows, .id=".id")

```

```{r plotGammaRecaptureoverall, caption="Number of parameters recaptured by each model, averaged across model runs", fig.width=7, fig.height=4}


ggplot(prop_recaptured_gam$fifty, aes(x=transition, y=mean_captured, fill=transition)) +
  geom_bar(stat="identity") +
  geom_bar(prop_recaptured_gam$ninety, stat="identity", alpha=0.5, mapping = aes(x=transition, y=mean_captured, fill=transition), inherit.aes = FALSE) +
  facet_grid(gammaid ~ beta_rate) +
  ylim(c(0,5)) +
  ggtitle("Mean # of parameters captured in 50% and 90% HPDI")

```

Most models return all parameters in the 90% HPDI, but a skinny gamma prior makes it harder to return parameters, especially when $\beta$ deviates from 1. 

Parameters are recaptured better when transition speed is fast, as long as the cutpoints prior isn't too skinny.

Which parameters are easier or harder to recapture?

```{r plotGammaRecapture, caption="Proportion of model runs where the HDPI contains the true parameter value"}

  
ggplot(dplyr::filter(prop_recaptured_by_param_gam,.id=="fifty"), mapping = aes(x=param, y=prop_inint, fill=transition)) +
  geom_bar(stat="identity", position="dodge") +
 geom_bar(dplyr::filter(prop_recaptured_by_param_gam, .id=="ninety"), stat="identity", position="dodge", alpha=0.5, mapping = aes(x=param, y=prop_inint, fill=transition), inherit.aes = FALSE) +
  facet_grid(gammaid  ~ transition + beta_rate) +
  ylim(c(0,1)) +
  ggtitle("Mean # of parameters captured in 50% and 90% HPDI") +
  theme(legend.position = "top")


```

$\beta$ and $c$ are never recaptured in the 50% HPDI when the real $\beta$ is slow, but they are usually recaptured in the 90% HPDI, except when the gamma prior on cutpoints is skinny.

A skinny gamma prior centered in between the transition points makes it difficult to recapture $\beta$ and $c$ across the board, unless $\beta$ is near 1 (See "medium" models). 

$\lambda$ (rate parameter for the exponential prior on $\beta$) choice doesn't seem to matter very much, but when transition speed is fast, a too loose (*i.e* small) $\lambda$ makes it harder to recapture $\beta$ and $c$. When transitions are slow, a too tight (*i.e* large) $\lambda$ might make estimates worse.

### Half transition points $h$

Both $h$ are always captured in the 90% HPDI, even when $\beta$ and $c$ are not recaptured. So the *ratio* between $\beta$ and $c$ is being captured, even when the actual values of those parameters aren't.

$h_2$ is estimated best (*i.e* in 50% HPDI) when transitions are fast and $h_1$ is estimated best when transitions are slow. This suggests 

Good estimates for $\beta$ and $c$ are correlated with worse estimates for $h$. This is a concerning tradeoff.

### Best prior parameter choices

Fat to middling gamma shape. 

Beta rate ($\lambda$) doesn't matter that much, but 2 works best.

The best choices for your prior parameters in this situation are a moderately loose $\lambda$ of 2 and a fat to middling gamma distribution.

### Caveats

I only tried centered the gamma prior on the cutpoints in between the two $h$. Could also try centering it closer to one transition or the other, or changing skew.

While models run on the slow transition dataset were less likely to have problems like with rhat, etc (given the default options of the stan call), it was also harder to estimate their parameters well. That's not ideal? But maybe rhat, etc problems can be fixed? I didn't investigate very thoroughly what was going wrong.



# Recapture parameters with induced dirichlet

Let's repeat this analysis using an induced dirichlet prior on cutpoints. I'll use the same simulated data as above.

I'll test the same $\lambda$ parameters as above. I'll also vary the anchor parameter in the induced dirichlet prior.

In his example, Betancourt sets the anchor to 0. I don't completely understand how the anchor works, so I'll try 5 and 10 as well and see what happens.

```{r paramsForInducedDirichletModels}

beta_rate <- c(1:3) # rate parameters for exponential prior on beta
anchor <- c(0, 5, 10) # different anchor parameters for induced dirichlet prior

# make a nice dataframe with all combinations params used to simulate data and model params used to try to recover those params

parframe_indir <- merge(simulation_input$pars, y=beta_rate) %>%
    rename(beta_rate=y) %>%
    merge(y=anchor) %>%
    rename(anchor=y) %>%
    mutate(h1 = simulation_input$h[1], h2 = simulation_input$h[2])
parframe_indir$modelid <- 1:nrow(parframe_indir)

# format parframe so it works with parLapply better
parlist_indir <- make_parframe_list(parframe_indir)

# model parameterizations
parameterizations_indir <- select(parframe_indir, anchor, beta_rate) %>%
  distinct() %>%
  arrange(anchor, beta_rate)

table_indir <- parframe_indir %>%
  select(transition, modelid, anchor, beta_rate) %>%
  group_by(transition, modelid, anchor, beta_rate) %>%
  tidyr::pivot_wider(names_from=transition, values_from=modelid) %>%
  tidyr::unite("modelids", c("fast", "medium", "slow"), sep=", ", remove=TRUE) %>%
  right_join(parameterizations_indir) %>%
  select(modelids, anchor, beta_rate)
```

I'm going to fit all three simulated datasets with each of the 3 rate prior parameters on beta and anchor parameters on the cutpoints.

```{r paramsTableIndir}
knitr::kable(table_indir, caption="Prior parameters - induced dirichlet prior models. modelids are unique identifiers for each combination of prior parameters and simulated datasets, in order from fast transition to slow transition dataset")
```

```{r indirRecapture, include=FALSE}


# # run all models, parallelized
# 
# # make a cluster using half your cores
# no_cores <- parallel::detectCores()/2
# cl <- parallel::makeCluster(no_cores)
# 
# # export the stuff you need to run on the cluster
# parallel::clusterExport(cl, c("fit_indir_model", "parlist_indir", "simdat"))
# parallel::clusterEvalQ(cl, c(library(rstan), library(StanHeaders)))
# 
# 
# for (i in 1:reps) {
#   fits_indir <- parallel::parLapply(cl, parlist_indir, function(x) {fit_indir_model(simdatlist = simdat, pars=x, groups=FALSE)})
#   saveRDS(fits_indir, file = paste0("induced_dirichlet/covar_runs/run", i, ".rds"))
#   rm(fits_indir)
#   gc()
# }
# 
# parallel::stopCluster(cl) #close the cluster

```


The Stan program implementing these models is

```{r}
writeLines(readLines("induced_dirichlet/dirichlet_covar.stan"))
```

```{r readIndirModels, warning=FALSE, cache=TRUE}

# pull in parameters and info on divergences, etc from saved stanfit objects
# very slow step. consider parallelizing to the extent your ram can handle

extracts_indir <- extract_pars_and_problems(path="induced_dirichlet/covar_runs/", parlist=parlist_indir, reps=10)
```

## Model check

Do a dangerously cursory check for model problems - divergences and issues with rhats and neff_ratios.

```{r indirModelCheck}
problems_indir <- extracts_indir$problems %>%
  purrr::map_dfr(bind_rows, .id=".id") %>%
  rename(run=.id) %>%
  group_by(modelid) %>%
  summarize(bad_proportion = n()/reps, bad_count = n(), divergences_mean=mean(divergences), bad_rhats_mean=mean(bad_rhats), bad_neff_mean=mean(bad_neff))
  
knitr::kable(problems_indir, caption = "Models with divergences, rhats > 1, or neff_ratios < 0.1 - induced dirichlet")

```

`r nrow(problems_indir)` out of 27 models had problems with Rhat or neff_ratios. Only model 4 had neff_ratio problems. Again, no divergences.

```{r indirPrettyModels}
noproblem_models_indir <- dplyr::filter(parframe_indir, !modelid %in% problems_indir$modelid) %>%
  arrange(desc(transition), anchor, beta_rate)
knitr::kable(noproblem_models_indir, caption="model parameterization-dataset combinations that never had an (very obvious) problem")
```

Slow transition models are least likely to have obvious problems.

```{r}
diag_indir <- diagnosis_framer(problems_indir, noproblem_models_indir)
```

```{r indirExtract}

# extract parameters from stan objects and pair with true param values
params_indir <- extracts_indir$pars %>%
  purrr::map_dfr(bind_rows, .id = ".id") %>%
  rename(run=.id) %>%
  left_join(diag_gam, by=c("modelid_true" = "modelid"))

```

The models have pretty consistent estimates across runs, even when there are problems with model diagnostics, but they are pretty consistently wrong in their estimates - parameters are usually underestimated. Cutpoints are quite sensitive to the anchor. $h_1$ is estimated much better than $h_2$, which is always underestimated

```{r plotIndirParams}

plot_densities_anchorxbeta <- function(modelpar, truepar) {
  plot <- ggplot(params_indir, aes(x=get(modelpar), group=interaction(run, modelid_true), fill=transition_true, colour=transition_true, linetype=modeldiag)) +
    geom_density(alpha=0.1) +
    geom_vline(aes(xintercept=get(truepar), colour = transition_true), alpha=1) +
    facet_grid(anchor_true ~ beta_rate_true) +
    ggtitle(strsplit(modelpar, split = "_model"), subtitle = "facet rows are anchor, cols are beta prior param")
  print(plot)
}

plot_densities_anchorxbeta("c.1_model", "c.1_true")
plot_densities_anchorxbeta("c.2_model", "c.2_true")
plot_densities_anchorxbeta("beta_model", "beta_true")

plot_densities_anchorxbeta("h1_model", "h1_true")
plot_densities_anchorxbeta("h2_model", "h2_true")

```

## Recapture rate

Of the 5 parameters ($\beta$, $c$ and derived $h$), how many are recaptured by the model?

```{r indirHDPI, message=FALSE, warning=FALSE, cache=TRUE}
# calculate whether true value is in HPDI

recaptured_indir <- params_indir %>% # are parameter estimates in the 50 or 90% HPDI?
  select(-modeldiag) %>%
  split(.$modelid) %>%
  which_params_recaptured()
prop_recaptured_indir <- calc_num_recaptured_overall(recaptured_indir, truepars=parframe_indir)
prop_recaptured_by_param_indir <- calc_recaptured_by_param(recaptured_indir, parframe_indir) %>%
  purrr::map_dfr(bind_rows, .id=".id")

```

```{r plotIndirRecaptureoverall, caption="Number of parameters recaptured by each model, averaged across model runs", fig.width=7, fig.height=4}

ggplot(prop_recaptured_indir$fifty, aes(x=transition, y=mean_captured, fill=transition)) +
geom_bar(stat="identity") +
geom_bar(prop_recaptured_indir$ninety, stat="identity", alpha=0.5, mapping = aes(x=transition, y=mean_captured, fill=transition), inherit.aes = FALSE) +
facet_grid(anchor ~ beta_rate) +
ylim(c(0,5)) +
ggtitle("Mean # of parameters captured in 50% and 90% HPDI")

```

Using an induced dirichlet prior on the cutpoints doesn't seem to work very well. Most parameters just aren't recaptured most of the time.

Which parameters are easier or harder to recapture?

```{r plotIndirRecapture}

ggplot(dplyr::filter(prop_recaptured_by_param_indir, .id=="fifty"), mapping = aes(x=param, y=prop_inint, fill=transition)) +
  geom_bar(stat="identity", position="dodge") +
  geom_bar(dplyr::filter(prop_recaptured_by_param_indir, .id=="ninety"), stat="identity", position="dodge", alpha=0.5, mapping = aes(x=param, y=prop_inint, fill=transition), inherit.aes = FALSE) +
  facet_grid(anchor  ~ transition + beta_rate) +
  ylim(c(0,1)) +
  ggtitle("Mean # of parameters captured in 50% and 90% HPDI", subtitle = "facet columns are transition and beta rate, rows are anchors") +
  theme(legend.position = "top")

```

When transitions are fast, no anchor allows $\beta$ and $c$ to be recaptured, but half transition points $h$ are (but $h_2$ isn't ever captured in 50% HPDI, only 90%). A larger anchor might help with this since cutpoints here are 20 and 30.

When transitions are medium speed, a large anchor allows all parameters to be recaptured in the 90% HPDI. Any anchor allows $h$ to be recaptured in the 90%, but $h_2$, again, is never captured in the 50%. Cutpoints are 10 and 15 here. A slightly larger anchor may help here as well.

When transitions are slow, $h2$ is never recaptured, but all other parameters are recaptured in the 90% HPDI. $\beta$ is recaptured better when anchors are lower, $c_1$ is captured better when anchor is 5, and $h_1$ is captured better when anchors are large (10).

### Interpretation

$c_1$ are `5`, `10`, and `20` for slow, medium, and fast transition datasets, respectively. Models were tried with anchors at `0`, `5`, and `10`. I think the dataset with a fast transition is so hard to fit a model to because an anchor of `10` is too far away from a $c_1$ at `20`. The model *does* recapture $c$:$\beta$. Since $c$ is underestimated, $\beta$ ends up being underestimated as well to preserve the ratio.

The induced dirichlet prior assumes the cutpoints are centered around the anchor. If I think reasonable ranges for my cutpoints are $\beta * h_1 = 1*10$ to $\beta * h_2 = 2*15$, then `20` might be a reasonable anchor. If I think the range is wider ($\beta * h_1 = 0.5 * 10$ to $\beta * h_2 = 3*15$), I'm not sure what a reasonable anchor might be. When $\beta=0.5$, the model never recaptured $h_2$. The anchor might struggle to pull small enough cutpoints - and have a small enough distance between cutpoints if $\beta$ is small and the anchor is large.

### Best prior parameter choices

I'm not sure! I think `20` to `25` is probably reasonable given what I know about the system.

# Gamma vs induced dirichlet prior on the cutpoints

I might be fighting more fit problems in Stan with a gamma prior than an induced dirichlet. Slow transition data fit with models with dirichlet priors are both less likely to have obvious fit problems (like sad rhats) and to return the correct parameter values for slow transition datasets. This is better than the situation with the gamma prior where slow transition datasets were also less likely to have obvious fit problems, but also to estimate parameters badly when it did so.

On the other hand, my data doesn't have a slow transition - but I could *make* it have a slow transition by re-scaling $x$.

<!-- Given what I found here, I think that a gamma prior is a better option for me. It definitely recaptured the parameters more often. I can use my domain specific knowledge around the potential range of $h$ and $\beta$ to choose a gamma distribution wide enough to cover a reasonable cutpoint range. -->

This model seems fussy and I'm concerned about how it's going to behave when I increase the complexity from $\beta x$ to something like $\beta x +  \alpha_{site} + \alpha_{prov} + \alpha_{clone})$.

## tl;dr

Ordinal logistic models with gamma priors have throw up all kinds of issues with the diagnostics and ordinal logistic models with dirichlet priors can't recapture the parameters (and have some rhat issues). If fit/stan diagnostic issues can be dealt with enough for the gamma prior, it may be workable. But things will likely explode when adding groups.



