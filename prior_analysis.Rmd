---
title: "Choosing priors in an ordinal logistic model"
author: "Susannah Tysor"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

    ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message=FALSE)
```

```{r packages}
library(dplyr)
library(ggplot2)
library(bayesplot)
library(rstan)
library(cowplot)
library(testthat)
library(parallel)
library(purrr)
```

```{r depends}
source('prior_analysis_functions.R')
```

```{r options}
rstan_options(auto_write=TRUE)
```

## Prior choice

Michael Betancourt thinks that an [induced dirichlet prior](https://betanalpha.github.io/assets/case_studies/ordinal_regression.html) on the cut points may be a good choice for ordinal logistic models.

When I modelled phenology using an ordinal logistic model, I used

* a gamma distribution for the prior on the cut points, $c$
* an exponential distribution distribution for the transition speed/slope, $\beta$
    * normal distributions for group effects on the slope

Normally, priors are bottom up - you choose a prior distribution for each parameter in your model. This is tricky for the cut points in an ordinal logistic model because they're defined on an abstract latent space where your domain expertise doesn't really apply. But you really want to choose a good prior in ordinal logistic models, because ordinal logistic models with covariates are inherently non-identifiable. (Because $\beta$ and $c$ depend on one another.)

Betancourt says that

> To avoid the non-identifiability of the interior cut points from propagating to the posterior distribution we need a principled prior model that can exploit domain expertise to consistently regularize all of the internal cut points at the same time. Regularization of the cut points, however, is subtle given their ordering constraint. Moreover domain expertise is awkward to apply on the abstract latent space where the internal cut points are defined.

He develops an induced dirichlet prior on the cut points. I want to compare the induced dirichlet and gamma priors on cut points for my situation. 

If I simulate data like mine using an ordinal logistic model, can I recapture the parameters using an ordinal logistic model with a gamma or induced dirichlet prior on the cut points?

At this point, I don't actually care about recapturing the cut points or beta. I do care about recapturing the transition points, $h$, though. $h$ are the points at which half of the population has transitioned from one state to another and is calculated as $h = \frac{c}{\beta}$. Eventually I will want to consider how different groups affect $h$ through deviations from the main slope $\beta$ (*e.g* $h = \frac{c}{\beta + \beta_{group_i}}$) and so estimating $\beta$s well might be more important then. But for now, let's just consider a an ordinal logistic model with a covariate/latent effect and no groups.

## Goals
1) When can I recapture parameters with a gamma prior?
2) When can I recapture parameters with an induced dirichlet prior?
3) Which one works best?

# Simulate data
First let's simulate some data that's kind of like mine.

I have three potential states $K$ and a latent effect/covariate $x$. $x$ is always positive and measurements are only made when $x$ is between `8` and `18`. Data collection begins when workers notice trees are approaching state `2` and stops as trees enter state `3`.

I simulate 3 datasets with slow ($\beta = 0.5$), medium ($\beta = 1$), and fast ($\beta = 2$) transition speeds. I want the transitions to occur at the same $x$ for both datasets. The halfway transition points $h$ will be at `x= 10` and `x=15`. $h$ is calculated with cut points $c$ and transition speed $\beta$, that is $h = \frac{c}{\beta}$ So while the transition points will stay the same in each simulated dataset, the cut points will differ. 

Cut points for the slow transition dataset will be at 5 and 7.5, at 10 and 15 for the medium and at 20 and 30 for the fast transition.

Here is what the first transition between states looks like for those transitions. My data is more like the fast transition - a phenological period with transitions of beta=0.5 would be occuring over an unrealistically long time period.

```{r transitions, fig.height=3, fig.width=4}

logistic2 <- function(x, beta, c) {
  y <- 1/(1+exp(-beta*x + c))
}

x <- c(0:20)
logisticexp <- data.frame(x, slow = logistic2(x, 0.5, 5), fast=logistic2(x, 2, 20), medium=logistic2(x, 1,10)) %>%
  tidyr::pivot_longer(cols=c(slow, medium, fast))

ggplot(logisticexp, aes(x=x, y=value, color=name)) +
  geom_line()
```

Now the real fake data:

```{r simulate_data}
simulation_input <- set_simulation_parameters(N=500)

simdat <- map(simulation_input$inputlist, simulate_data, groups=FALSE)

plot_simulated_data(simdat, simulation_input$inputlist)

```

## Attempt to recapture parameters

Now I want to try to recapture parameters.

I need to know how good I have to be at choosing the shape and rate parameters for the gamma prior on the cut points to recapture the "true" slope and cut points parameters as well. (The mean of the gamma is the shape divided by the rate.)  

Choosing where to center the gamma isn't obvious since cut points are on that "abstract latent space." Since $c = \beta h$, though, we have some idea of what that is. $x=12.5$ is smack dab in the middle of our transition points $h$, which we can roughly estimate from the data. It would be nice to be able to use this number that we know when choosing the parameters for the gamma prior. 

If we center a gamma distribution on state `2`, how much wiggle room does it need to find the cut points? When $\beta=0.5$, `12.5` is `6.25` on the cut point space and when $\beta=2$, `12.5` is `25` on the abstract latent space. The larger $\beta$ is, the bigger and further apart the cut points are. The smaller $\beta$ is, the smaller and closer together the cut points are. Since we don't know $\beta$ in advance, we probably have to choose a fat gamma so we can access the full range of values where cut points could be. On the other hand, we might end up estimating combinations of $c$ and $\beta$ that get $h$ right, but both $c$ and $\beta$ very wrong.

I also want to understand how the exponential prior on $\beta$ affects the ability of the model to recapture parameters. I'll try rates of 1,2,3.


### Gamma prior with covariate
```{r paramsForGammaModels}

beta_rate <- c(1:3) # rate parameters for exponential prior on beta

# shape and rate parameters for gamma prior on cut points, all centered on mean transition point

factors <- list(fat=0.10, middling=0.25, skinny=1)

pars_gamma <- purrr::map(factors, make_gamma_pars, h=simulation_input$h)  

#plot gamma priors
pars_gamma %>%
  # format for plotting
  map(function(x) {rgamma(1000, x$shape, x$cut_rate)}) %>%
  bind_cols() %>%
  tidyr::pivot_longer(cols = names(factors)) %>%
  # plot
  ggplot(aes(x=value, fill=name)) +
  geom_density(alpha=0.5) +
 # facet_grid(name ~ .) +
  ggtitle("Gamma priors all centered on 10", subtitle = "with different shape and rate params")

# make a nice dataframe with all combinations params used to simulate data and model params used to try to recover those params
parframe_gam <- pars_gamma %>%
  purrr::map_dfr(.f=bind_cols, .id=".id") %>%
  merge(beta_rate) %>%
  dplyr::rename(gammaid=.id, beta_rate=y) %>%
  merge(simulation_input$pars)
parframe_gam$h1 <- simulation_input$h[1]
parframe_gam$h2 <- simulation_input$h[2]
parframe_gam$modelid <- 1:nrow(parframe_gam) #label the models
```

So there are 3 simulated datasets - 1 for each of three transition speeds. I'm going to try to fit them both with 3 rates for the $\beta$'s exponential prior and 3 shape and rate combinations for the cut points' gamma prior, a total of 27 model runs. 

```{r paramsTableGamma}
knitr::kable(parframe_gam, caption="Model parameterizations used to try to recapture params")
```


```{r GammaRecapture, include=FALSE, cache=TRUE, warning=TRUE}
# Parallel code actually executes in parallel when running this chuck as a chunck with ctrl+shift+enter. It does not if you run it line by line. I don't know why. 

# format parframe so it works with parLapply better
parlist_gam <- make_parframe_list(parframe_gam)


# run all models, parallelized

# make a cluster using half your cores
no_cores <- parallel::detectCores()/2
cl <- parallel::makeCluster(no_cores)

# export the stuff you need to run on the cluster
parallel::clusterExport(cl, c("fit_gamma_model", "parlist_gam", "simdat"))
parallel::clusterEvalQ(cl, c(library(rstan), library(StanHeaders)))

# fit the models
# run each model 30 times
fits_gam_list <- list()
reps <- 30

for (i in 1:2) {
  fits_gam <- parallel::parLapply(cl, parlist_gam, function(x) {fit_gamma_model(simdatlist = simdat, pars=x, groups=FALSE)})
  fits_gam_list[[i]] <- fits_gam
}

parallel::stopCluster(cl) #close the cluster

```
### Model check
Do a naive check for model problems - divergences and issues with rhats and neff_ratios.
```{r modelcheck}

bad_models <- map_dfr(fits_gam_list, check_list_of_models, .id = ".id") %>%
  rename(iteration = .id) %>%
  mutate(all_bads = divergences + bad_rhats + bad_neff) %>%
  filter(all_bads > 0) %>%
  select(-all_bads) %>%
  group_by(modelid) %>%
  summarize(bad_proportion = n()/reps)

knitr::kable(bad_models, caption = "Models with divergences, rhats > 1, or neff_ratios < 0.1")

```

```{r gammaExtract}

# extract parameters from stan objects pair with true
#params_gam <- bind_true_model_pars(fits_gam, parlist_gam)
params_gam <- map(fits_gam_list, bind_true_model_pars, parlist=parlist_gam)

```

```{r gammaParPlots, fig.height=2}
#plot params and diffs
#map(paramsgam, parplot)
# map(paramsgam, posterior_differencer) %>%
#     map2(.y=paramsgam, .f=diffplotter)
```

### Recapture

Of the 5 parameters ($\beta$, $c$ and derived $h$), how many are recaptured by the model?
```{r GammaHDPI}
# calculate whether true value is in HPDI
# MAKE THIS WORK WITH A LIST OF LISTS
in50 <- map(params_gam, calc_HPDI, prob=0.5)

in75 <- map(params_gam, calc_HPDI, prob=0.75)

in90 <- map(params_gam, calc_HPDI, prob=.90)

# recaptured parameters
perform50 <- map_df(in50, bind_rows, .id="modelid") %>%
  dplyr::mutate(modelid=as.integer(modelid)) %>%
  full_join(parframe_gam) 
perform90 <- map_df(in90, bind_rows, .id = "modelid") %>%
  dplyr::mutate(modelid=as.integer(modelid)) %>%
  full_join(parframe_gam)

# proportion of parameters recaptured
prop_recaptured50 <- perform50 %>%
  group_by(modelid) %>%
  summarise(captured = sum(inint)) %>%
  full_join(parframe_gam) %>%
  arrange(beta, desc(captured))

prop_recaptured90 <- perform90 %>%
  group_by(modelid) %>%
  summarise(captured = sum(inint)) %>%
  full_join(parframe_gam) %>%
  arrange(beta, desc(captured))

#knitr::kable(prop_recaptured50, caption = "Proportion of params recaptured in 50% HPDI")
#knitr::kable(prop_recaptured90, caption = "Proportion of params recaptured in 90% HPDI")
```

```{r plotGammaRecaptureoverall}
ggplot(prop_recaptured50, aes(x=gammaid, y=as.factor(beta_rate), fill=captured)) + 
  geom_tile(colour="white") +
  geom_text(aes(label=modelid)) +
  facet_wrap("transition", scales="free") +
  ggtitle("Proportion of parameters recaptured in 50% HPDI", subtitle = "faceted by transition speed in simulated dataset") +
  scale_fill_viridis_c(limits=c(0,1)) +
  ylab("beta_rate")

ggplot(prop_recaptured90, aes(x=gammaid, y=as.factor(beta_rate), fill=captured)) + 
  geom_tile(colour="white") +
  geom_text(aes(label=modelid)) +
  facet_wrap("transition", scales="free") +
  ggtitle("Proportion of parameters recaptured in 90% HPDI",subtitle = "faceted by transition speed in simulated dataset") +
  scale_fill_viridis_c(limits=c(0,1)) +
  ylab("beta_rate")
```

Most models return all parameters in the 90% HPDI, but a skinny gamma prior makes it harder to return parameters, especially when $\beta$ deviates from 1. 

Which parameters are easier or harder to recapture?



```{r plotGammaRecapture}
#knitr::kable(perform50, caption = "Ability of the different models to recapture parameters in 50% HPDI. Each model only run only once.")

ggplot(perform50, aes(x=params, y=as.factor(modelid), fill=inint)) + 
  geom_tile(color="black") +
  scale_fill_viridis_d()  +
  geom_text(aes(label=paste(gammaid, beta_rate, sep="\n")), size=2.5) +
  facet_wrap("beta", scales="free") +
  ggtitle("Parameters recaptured in 50% HPDI", subtitle = "faceted by transition speed (beta) in simulated dataset\n with gamma shape and beta rate labels") +
  theme(legend.position = "top") +
  ylab("model id")
  

#knitr::kable(perform90, caption = "Ability of the different models to recapture parameters in 90% HPDI. Each model only run only once, don't shoot me.")

ggplot(perform90, aes(x=params, y=as.factor(modelid), fill=inint)) + 
  geom_tile(color="black") +
  scale_fill_viridis_d()  +
  geom_text(aes(label=paste(gammaid, beta_rate, sep="\n")), size=2.5) +
  facet_wrap("beta", scales="free") +
  ggtitle("Parameters recaptured in 90% HPDI", subtitle = "faceted by transition speed (beta) in simulated dataset\n with gamma shape and beta rate labels") +
  theme(legend.position = "top") +
  ylab("modelid")

```

A skinny gamma prior centered in between the transition points doesn't work unless the slope is near one (putting the mean of the transition points and the mean of the cutpoints in the same ballpark). (See models 24, 15, 6)

A fast transition underlying speed in the data combined with a fat gamma prior on the cutpoints and a very loose prior on beta make it harder to recapture parameters (see model 1).

All parameters except the second transition point ($h2$) are recaptured in the 90% HPDI. 

$h$ is *never* recaptured in the 50% HPDI, and, while $h1$ is recaptured in all 90% HPDIs, $h2$ never is. That's a bummer, since that's what I actually want to know. Also, how can $c_2$ and $\beta$ be captured, but $h_2$ not be when $h_2 = \frac{c_2}{\beta}$?

The best choices for your prior parameters in this situation is a moderately loose $\beta_rate$ of 2 and a fat to middling gamma distribution.

```{r plotGammaParams, fig.height=2}
map(params_gam[c(1, 5, 11, 15, 21, 22, 23)], parplot)
```

### Dirichlet prior with covariate



```{r dirichlet recapture}
fitdir <- stan(file='dirichlet prior/dirichlet/dirichlet_covar.stan', data=input_data_for_model, chains=4)

paramsdir <- data.frame(rstan::extract(fitdir))
summary(paramsdir)

pdir <- parplotter("dirichlet", paramsdir)
pdir

diffplotter(model_params=paramsdir, priorgroups="induced dirichlet prior, no groups")

```



When the transition between states is relatively rapid, the dirichlet struggles a bit. It's really underestimating both cut points and beta - but getting the inflection points quite right. This occurs with a $\beta$ of exp(2) and 1. If the $\beta$ prior is too loose (exp(0.5)) or too tight (exp(4)), the induced dirichlet fails.

The anchor point in the dirichlet also really matters to cutpoint estimation. Anchors much lower than the first true cutpoint really push the cutpoint estimate down. Anchors closer to the midpoint of the true cutpoint do better at estimating cut points, but worse at estimating transitions.





Cut point anchor medium

```{r exponential}
hist(rexp(1000,0.5), breaks=30)
hist(rexp(1000,2), breaks=30)
hist(rexp(1000,4), breaks=30)
```

With rapid transitions, the model with the induced dirichlet can do ok at estimating the inflection points, but if you aren't making good choices about $\beta$ it will fail hard.

What if the anchor point is closer to the first transition? START HERE maybe already done too tired.

```{r compare gamma and dirichlet}


ddir <- diffplotter(paramsdir, priorgroups="dirichlet prior, no groups")
dgam <- diffplotter(paramsgam, priorgroups = "gamma prior, no groups")
ggarrange(dgam, ddir, ncol=1)

ggarrange(pgam, pdir, ncol=1)

```

Gamma prior model does underestimate both cut points and beta, but not as much as the dirichlet does. However, the inflection points/half transition points (h1 and h2) are maybe estimated a bit better by the dirichlet?

## Conclusions on gamma vs induced dirichlet
For my case - where cut points and beta are positive and most data is likely to be collected around state 2.




# Covariate and a group

What if our data has groups that have different effects? 

## Group effects just a little smaller than $\beta$ 
What if the effects are around the same size as beta and beta is relatively small (which makes the model harder to fit)?

Let's simulate 7 groups with `N=500` observations for each group. Effects for the groups are simulated from a normal distribution with `mean=0` and `sd=0.25`. They're deviations from the population mean.

```{r group simulation}

N = 500*7
G = 7
K = 3

# parameters
c <- c(3,7) # cut points
beta = 0.5

# simulate individual group effects
gbeta_mu <- 0
gbeta_sd <- 0.25

gbeta_vec <- rnorm(G, mean=gbeta_mu, sd=gbeta_sd) # group effects
gbeta <- sort(sample(gbeta_vec, size=N, replace=TRUE)) # assign a group effect to every observation
gid <- as.numeric(as.factor(gbeta)) # label the groups
groupeffects <- data.frame(gbeta, gid) %>%
  unique()

h1 <- c[1]/(beta+groupeffects$gbeta)
h2 <- c[2]/(beta+groupeffects$gbeta)
x <- rtnorm(n=N, mean=mean(c(h1,h2)), sd=mean(h)/4, min=0, max=20) #covariate centered around transitions
hist(x)

input_data_for_simulation <- list("N" = N, "K"=K, "G"=G, "x" = x, "c" = c, "beta"=beta, "gbeta"= gbeta)

simu <- stan(file='dirichlet prior/covar_group_sim.stan', iter=1, chains=1,
             algorithm="Fixed_param", data=input_data_for_simulation)

simu_params <- rstan::extract(simu)

input_data_for_model <- list("N" = N, "K" = K, "G"=G, "x" = x, "GID"= gid, "y" = array(simu_params$y[1,]))

table(input_data_for_model$y)
plot(input_data_for_model$x, input_data_for_model$y)

groupdata <- data.frame(x=x, y=input_data_for_model$y, GID=gid)
ggplot(groupdata, aes(x=x, color=as.factor(y))) +
    stat_ecdf() +
    facet_wrap("GID")
```

### Gamma recapture 
Can a model with the gamma prior recapture the parameters used to simulate the data?

The group effect prior ~ $\mathrm{normal}(0,sigma_{group})$ with $sigma_{group} ~ \mathrm{exponential}(4)$

```{r gamma recapture group}
fitgam <- stan(file='dirichlet prior/gamma/gamma_covar_group.stan', data=input_data_for_model, chains=4, control = list(adapt_delta=0.95, max_treedepth=11), iter=5000)

paramsgamg <- data.frame(rstan::extract(fitgam)) 

summary(paramsgamg)

# function to make a graph for a group of parameter values comparing them to the "true" value


parplot <- function(label, pardf) {
  cutpointsp <- mcmc_areas(pardf, regex_pars="c") +
    geom_vline(xintercept=c)
  betap <- mcmc_areas(pardf, pars="beta") +
    geom_vline(xintercept=beta)
  betagp <- mcmc_areas(pardf, regex_pars="betag") +
    geom_vline(xintercept = groupeffects$gbeta)
  hp <- mcmc_areas(pardf, regex_pars = "fstart") +
    geom_vline(xintercept=h1)
  ggpubr::ggarrange(cutpointsp, betap, betagp, hp, labels=label)
}

pgam <- parplot("gamma, beta=0.5", paramsgamg)
pgam

diffplotter(model_params=paramsgam, priorgroups="gamma prior, no groups")
```

### These notes are for data simulated around base population h (with group effects subtracted out), not individual group h's.
There was a divergent transition, treedepth warning, and ESS warnings with the default adapt delta, so I increased it to 0.9. Still problems. 

Trying 0.95. No more divergences, but 1 transition after warmup exceeded max treedept and still some bulk ESS problems. $\beta$ parameters are all overestimated.

Keeping adapt_delta at 0.95 and increasing max treedepth to 11 leaves only ESS problems. Estimates are not great - all beta parameters are overestimated and there's no differentiation between them. Not ideal.

Attempting an increase in iterations - that kills all the Stan errors! Unfortunately, $\beta$, $\beta_g$s, and $sigma_group$ are still really overestimated and broad (just bad!). And none of the $\beta_g$s are negative.

On the one hand, this is not great from a model perspective. OTOH, these are pretty unrealistic group effects - ones that shift fstart quite a lot.

### These notes are for data simulated around an h based on the entire population, including groups

Tail ESS errors with iter at 4000, increased to 5000. No more warnings. But how does estimation go? Still badly. Fails to distinguish between groups, overestimates beta, underestimates cut points.


## Group effects about 1/4 the size of beta.
Is it the relative size of $beta$ and $betag$s that's the problem or the absolute size of $betag$? Let's bump up $beta$ to the easier to fit 2 (more rapid state transitions) and see how our model does.

```{r group simulation big beta}

N = 500*7
G = 7
K = 3

# parameters
c <- c(5,10) # cut points
beta = 2

# simulate individual group effects
gbeta_mu <- 0
gbeta_sd <- 0.25

gbeta_vec <- rnorm(G, mean=gbeta_mu, sd=gbeta_sd) # group effects
gbeta <- sort(sample(gbeta_vec, size=N, replace=TRUE)) # assign a group effect to every observation
gid <- as.numeric(as.factor(gbeta)) # label the groups
groupeffects <- data.frame(gbeta, gid) %>%
  unique()

h1 <- c[1]/(beta+groupeffects$gbeta)
h2 <- c[2]/(beta+groupeffects$gbeta)
x <- rtnorm(n=N, mean=mean(c(h1,h2)), sd=mean(h), min=0, max=20) #covariate
hist(x, breaks=30)
     

input_data_for_simulation <- list("N" = N, "K"=K, "G"=G, "x" = x, "c" = c, "beta"=beta, "gbeta"= gbeta)

simu <- stan(file='dirichlet prior/covar_group_sim.stan', iter=1, chains=1,
             algorithm="Fixed_param", data=input_data_for_simulation)

simu_params <- rstan::extract(simu)

input_data_for_model <- list("N" = N, "K" = K, "G"=G, "x" = x, "GID"= gid, "y" = array(simu_params$y[1,]))

table(input_data_for_model$y)
plot(input_data_for_model$x, input_data_for_model$y)

groupdata <- data.frame(x=x, y=input_data_for_model$y, GID=gid)
ggplot(groupdata, aes(x=x, color=as.factor(y))) +
    stat_ecdf() +
    facet_wrap("GID")
```


```{r gamma recapture group big beta}
fitgam <- stan(file='dirichlet prior/gamma/gamma_covar_group.stan', data=input_data_for_model, chains=4, control = list(adapt_delta=0.95, max_treedepth=11), iter=4000)

fitgam <- stan(file='dirichlet prior/gamma/gamma_covar_group.stan', data=input_data_for_model, chains=4)

paramsgamg <- data.frame(rstan::extract(fitgam)) 

summary(paramsgamg)

# function to make a graph for a group of parameter values comparing them to the "true" value


pgam <- parplot("gamma, beta=2", paramsgamg)
pgam

```

Model doesn't give any obvious warnings, but  it overestimates cut points, underestimates beta, and dramatically overestimates betags.

If I change the priors from `cutpoints ~ gamma(10,1)` to `gamma(5,1)` and reduce the sigma_group prior to `exp(5)` from `exp(4)`, it does the same.

If I bump up cut points so that the covariate distribution isn't bumping up against 0, beta is underestimated and group betas really overestimated.

If I strongly constrain the sigma_group prior to `exp(10)`, it doesn't change anything.

If I use a uniform distribution for the covariate, beta is underestimated, group betas are still really overestimated (maybe even worse!) and not differentiated.

Why are all the group effect betas positive and undifferentiated? Does the model need more data? This run took... a really long time. 44109.6 seconds. And it had warnings... And the fit was even worse.

```
Warning messages:
1: There were 393 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 11. See
http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded 
2: Examine the pairs() plot to diagnose sampling problems
```

Can the group effects be recovered if I explicity specify the sd as 0.25 in the model? No it cannot. That's depressing.

What if beta is bigger compared to group effects - like 5? Even more disastrous. Beta is SO overestimated.

What if I drop beta to 1? Still garbage with betas all so overestimated.

I think I might be simulating my data kind of poorly - or maybe this model isn't good about simulating this data? Or maybe beta should be bigger? I think the transitions between states occurs faster in my data.

I could also try a really big beta - like 10.
