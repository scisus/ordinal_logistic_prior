---
title: "Choosing priors in an ordinal logistic model"
author: "Susannah Tysor"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

    ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message=FALSE)
```

```{r packages}
library(dplyr)
library(ggplot2)
library(bayesplot)
library(rstan)
library(cowplot)
library(testthat)
library(parallel)
library(purrr)
```

```{r options}
rstan_options(auto_write=TRUE)
```

```{r functions}

# generate samples from a truncated normal distribution. n = how many samples, mean and sd are mean and sd, and min and max are the limits of the distribution/truncation points. possibly my first ever use of while loops. Don't @ me.
rtnorm <- function(n, mean, sd, min, max) {
    x <- rnorm(n, mean=mean, sd=sd)
    x <- x[x >= min & x <= max]
    while(length(x) < n) {
        newx <- rnorm(1, mean=mean, sd=sd)
        while(newx <= min | newx >= max) {
            newx <- rnorm(1, mean=mean, sd=sd)
        }
        x <- c(x, newx)
    }
    length(x)==n
    return(x)
}

# simulate data from an ordinal logistic model and format it as input for stan. input is a list for stan, as is output. groups is TRUE or FALSE
simulate_data <- function(input, groups) {
  if (isTRUE(groups)) {
    simu <- rstan::stan(file='dirichlet prior/covar_group_sim.stan', iter=1, chains=1, algorithm="Fixed_param", data=input)
  } else {
    simu <- rstan::stan(file='dirichlet prior/covar_sim.stan', iter=1, chains=1, algorithm="Fixed_param", data=input)
  }
  
  simu_params <- rstan::extract(simu)
  
  input_data_for_model <- list("N" = input$N, "K" = input$K, "G"=input$G, "gid" = input$gid, "x" = input$x, "y" = array(simu_params$y[1,]))
  return(input_data_for_model)
}


# # append the correct shape (gamma prior on cutpoints) and rate (exponential prior on beta) parameters to the simulated data list and fit a model with a gamma prior. simdat is a list of datasets simulated from stan models and pars is a vector of additional parameters. Relies on global params beta_slow and beta_fast.
# fit_gamma_model <- function(simdatlist, pars) {
#     #choose whether to use data simulated with a rapid or slow transition
#     if (pars$beta == beta_slow) {
#         simdat <- simdatlist$simdat_slow
#     }
#     if (pars$beta == beta_fast) {
#         simdat <- simdatlist$simdat_fast
#     }
#     #extract parameters for prior distribtuions
#     simdat$shape <- pars$shape
#     simdat$rate <- pars$beta_rate
# 
#     #fit the model
#     fitgam <- stan(file='dirichlet prior/gamma/gamma_covar.stan', data=simdat, chains=4)
#     return(fitgam)
# }

## append a label (string) to all columnnames in a dataframe (x)
label_names <- function(x, label) {
    colnames(x) <- paste0(colnames(x), "_", label)
    return(x)
}

# function to plot modeled parameters with label being a string to label the graph (usually prior and whether groups were included and pardf is the modeled parameter dataframe) trueparams is a one row dataframe of true parameters. h is a global parameter have fun!
parplot <- function(pars) {
    cutplot <- mcmc_intervals(pars, regex_pars="c.\\d_model") +
        geom_vline(xintercept=c(unique(pars$c.1_true), unique(pars$c.2_true)))

    betaplot <- mcmc_intervals(pars, pars="beta_model") +
        geom_vline(xintercept=unique(pars$beta_true))

    h1plot <- mcmc_intervals(pars, regex_pars = "h1_model") +
        geom_vline(xintercept=unique(pars$h1_true))
    h2plot <- mcmc_intervals(pars, regex_pars = "h2_model") +
        geom_vline(xintercept=unique(pars$h2_true))
    allplot <- cowplot::plot_grid(cutplot, betaplot, h1plot, h2plot,
                                  nrow=1, ncol=4,
                                  labels=paste("model", unique(pars$modelid_true),
                                               "rate=", unique(pars$beta_rate_true),
                                               "shape=", unique(pars$shape_true)))
    print(allplot)
}

# function to calculate the difference between modeled parameters (in the model_params dataframe) and true parameters (h is globally declared). Where true params is a dataframe
posterior_differencer <- function(pars) {
    c1_diff <- pars$c.1_model - pars$c.1_true
    c2_diff <- pars$c.2_model - pars$c.1_true
    h1_diff <- pars$h1_model - h[1]
    h2_diff <- pars$h2_model - h[2]
    beta_diff <- pars$beta_model - pars$beta_true
    diffframe <- data.frame(c1_diff, c2_diff, h1_diff, h2_diff, beta_diff)
    return(diffframe)
}

# function to plot histograms of differences between true params and modeled params (model_params dataframe).
diffplotter <- function(diffs, pars) {
    #diffs <- posterior_differencer(pars)
    cuts <- mcmc_intervals(diffs, regex_pars = "c") +
        ggtitle("", subtitle = "differences between modeled and true params")+
        xlim(c(-15,15))
    opars <- mcmc_intervals(diffs, pars=c("h1_diff", "h2_diff", "beta_diff")) +
        xlim(c(-1,1))
    cowplot::plot_grid(cuts, opars, labels=paste("model", unique(pars$modelid_true),
                                                 "rate=", unique(pars$beta_rate_true),
                                                 "shape=", unique(pars$shape_true)))
}


```


When I first built an ordinal logistic model for lodgepole pine pollination phenology, I used

* a gamma distribution for the prior on the cutpoints
* an exponential distribution distribution for the transition speed/slope $\beta$, and 
    * normal distributions for group effects on the slope

Michael Betancourt thinks that an induced dirichlet prior may be a [good choice](https://betanalpha.github.io/assets/case_studies/ordinal_regression.html) for ordinal logistic models.

Normally, priors are bottom up - you choose a distribution for each prior. This is tricky for the cutpoints in an ordinal logistic model because they're defined on an abstract latent space so you can't easily use your domain expertise, but a good prior is incredibly important in these models, because ordered logistic models with covariates are inherently non-identifiable. (Because transition speeds ($\beta$) and cutpoints ($c$) depend on one another.)

Betancourt says that

> To avoid the non-identifiability of the interior cut points from propagating to the posterior distribution we need a principled prior model that can exploit domain expertise to consistently regularize all of the internal cut points at the same time. Regularization of the cut points, however, is subtle given their ordering constraint. Moreover domain expertise is awkward to apply on the abstract latent space where the internal cut points are defined.

That's what the induced dirichlet should do. Does it work for my model and how does it compare to a gamma prior on cutpoints chosen using domain specific knowledge?

# Goals

1) Understand the dynamics of the gamma and induced dirichlet priors and 2) compare them

# Simulate data
First let's simulate some data that's more or less like mine.

We have three potential states $K$ and a latent effect/covariate $x$. Data is most likely to be collected around state 2. $x$ is always positive.

I'll simulate three datasets - one with a fast transition speed ($\beta = 2$), a medium transition speed ($\beta = 1$) and one with a slow transition speed ($\beta = 0.5$). I want the transitions to occur at the same $x$ for both datasets. The halfway transition points $h$ will be at $x= 8$ and $x=12$. $h=\frac{c/\beta$}

Here is what a slow, medium, and fast transition look like. My data is more like the fast transition - a phenological period with transitions of $\beta=0.5$ would be occuring over a really long time period - months instead of days or weeks.

```{r transitions, fig.height=4}

logistic2 <- function(x, beta, c) {
  y <- 1/(1+exp(-beta*x + c))
}

x <- seq(from=0, to=20, by=0.25)
logisticexp <- data.frame(x, slow = logistic2(x, 0.5, 6), fast=logistic2(x, 2, 24), medium=logistic2(x, 1,12), faster=logistic2(x, 5, 60)) %>%
  tidyr::pivot_longer(cols=c(slow, medium, fast, faster))

l1 <- ggplot(logisticexp, aes(x=x, y=value, color=name)) +
  geom_line() +
  ggtitle("Logistic functions centered at x=12 with varying betas")
l2 <- ggplot(logisticexp, aes(x=x, y=value, color=name)) +
  geom_line() +
  xlim(c(8,16))
cowplot::plot_grid(l1, l2, nrow=1)
```

Of course, $x$ can be rescaled any way you want, so you can make transitions slower or faster depending on how you smoosh or stretch $x$.

## Simulate datasets

```{r set simulation parameters}
N <- 500
K <- 3

beta <- data.frame(transition = c("slow", "medium", "fast"), beta=c(0.5, 1, 2))
cutpoints <- data.frame(c.1= c(4,8,16), c.2=c(6, 12, 24), transition=c("slow", "medium", "fast"))

simu_pars <- merge(beta, cutpoints)

# half transition points, engineered to be identical all transitions
h1 <- unique(simu_pars$c.1/simu_pars$beta )
h2 <- unique(simu_pars$c.2/simu_pars$beta )
h <- c(h1, h2)

# covariate over full range of heat accumulation (risto scale) Jan-Julyish
x <- rtnorm(n=N, mean=mean(h), sd=2, min=0, max=20) #covariate

testthat::test_that("half transition points identical", {
  testthat::expect_equal(length(h), 2)
})
```

```{r simulateData, fig.height="300px"}

inputs_for_sim <- split(simu_pars, simu_pars$transition) %>%
  map(.f=function(y) {list("N" = N, "K" = K, "c" = c(y$c.1, y$c.2), "beta"=y$beta, "h" = h, "x" = x)})

simdat <- map(inputs_for_sim, simulate_data, groups=FALSE)

# plot simulated data for a sanity check.
simplot <- function(datalist) {
    inputdf <- data.frame(datalist)
    p1 <- ggplot(inputdf, aes(x=x, y=y)) +
        geom_jitter(shape=1, height=0.2) +
        geom_vline(xintercept = h) +
        ggtitle("Simulated data with cutpoints")
    print(p1)
    p2 <- ggplot(inputdf, aes(x=x, colour=as.factor(y))) +
        stat_ecdf() +
        geom_vline(xintercept=h) +
        theme(legend.position = "none") +
        ggtitle("Cumulative x for 3 states")
    print(p2)
    p3 <- cowplot::plot_grid(p1,p2)
    print(p3)
}

# plot simulated data
simdf <- purrr::map(simdat, .f = function(x) {x[c("x", "y")]}) %>%
  purrr::map_dfr(.f = bind_rows, .id=".id") 

p1 <- ggplot(simdf, aes(x=x, y=y)) +
  geom_jitter(shape=1, height=0.1, alpha=0.5) +
  geom_vline(xintercept = h) +
  ggtitle("Simulated data with cutpoints") +
  facet_grid(.id ~ .)

p2 <- ggplot(simdf, aes(x=x, colour=as.factor(y))) +
  stat_ecdf() +
  geom_vline(xintercept=h) +
  theme(legend.position = "none") +
  ggtitle("Cumulative x for 3 states") +
  facet_grid(.id ~ .)

cowplot::plot_grid(p1, p2, ncol=2)

```

## Recapture parameters: Gamma

Now I want to try to recapture parameters using an ordinal logistic model with a gamma prior on the cutpoints and an exponential prior on the transition speeds.

I want to know how good I have to be at choosing parameters for the gamma prior on the cutpoints to recapture parameters. Ideally, I can anchor the prior **somewhere** near the cutpoints and give it enough spread to capture them. This is hard for the reasons explained by Betancourt above.

I'm going to try centering the gamma distribution at $x$'s where stage 2 is most likely and giving it varying degrees of spread. The further $\beta$ is from 1, the worse this will work. 

I also want to understand how the exponential prior on beta affects the ability to recapture parameters. I'll try rates of 1,2,3.


### Gamma prior with covariate
```{r paramsForGammaModels}

beta_rate <- c(1:3) # rate parameters for exponential prior on beta

# shape and rate parameters for gamma prior on cutpoints, all centered on mean transition point

# make shape and rate parameters for a gamma distribution centered on the mean of h (a vector) with spread scaled by a factor (scaler). Larger factors make the distribution skinnier and smaller ones make it fatter. "h" are the half transition points, so the mean is basically the midpoint of your state 2 in a 3 state system when you have data like mine.
make_gamma_pars <- function(factor, h) {
  center <- mean(h)
  shape <- mean(h) * factor
  cut_rate <- shape/center
  sr <- data.frame(shape, cut_rate)
  return(sr)
}

factors <- list(fat=0.25, normal=1, skinny=2)

gamma_pars <- purrr::map(factors, make_gamma_pars, h=h)  
gamma_parsdf <-purrr::map_dfr(gamma_pars, .f=bind_cols, .id=".id") 

sr <- gamma_pars %>%
  map(function(x) {rgamma(1000, x$shape, x$cut_rate)}) %>%
  bind_cols() %>%
  tidyr::pivot_longer(cols=names(factors))

# plot gamma priors
ggplot(sr, aes(x=value, fill=name)) +
  geom_density() +
  facet_grid(name ~ .) +
  ggtitle("Gamma priors all centered on 10", subtitle = "with different shape and rate params")


# make a nice dataframe with all combinations params used to simulate data and model params used to try to recover those params
parframe_gam <- merge(gamma_parsdf, beta_rate) %>%
  dplyr::rename(gammaid=.id, beta_rate=y) %>%
  merge(simu_pars)
parframe_gam$h1 <- h[1]
parframe_gam$h2 <- h[2]
parframe_gam$modelid <- 1:nrow(parframe_gam) #label the models


# shape_slow <- mean(cutpoints_slow)
# shape_slow_small <- shape_slow/2
# shape_slow_big <- shape_slow*2
# 
# slowshapes <- c(shape_slow, shape_slow_small, shape_slow_big)
# 
# shape_fast <- mean(cutpoints_fast)
# shape_fast_small <- shape_fast/2
# shape_fast_big <- shape_fast*2
# 
# fastshapes <- c(shape_fast, shape_fast_big, shape_fast_small)

# dataframes to hold parameters
# slowframe_gam <- data.frame(beta = beta_slow, shape = slowshapes, beta_rate=beta_rate, c.1 = cutpoints_slow[1], c.2=cutpoints_slow[2], h1=h[1], h2=h[2]) %>%
#   tidyr::pivot_longer(cols=)
# fastframe_gam <- data.frame(beta=beta_fast, shape= fastshapes, beta_rate=beta_rate, c.1 = cutpoints_fast[1], c.2=cutpoints_fast[2], h1=h[1], h2=h[2])
# parframe_gam <- rbind(slowframe_gam, fastframe_gam) %>%
#   tidyr::expand(tidyr::nesting(beta, c.1, c.2, shape), h1, h2, beta_rate)

# parframe_gam <- dplyr::select(parframe_gam, modelid, beta, c.1, c.2, h1, h2, shape, beta_rate)

knitr::kable(parframe_gam, caption="model configurations used to try to recapture params")
```

So there are 3 simulated datasets - 1 for each of three transition speeds. I'm going to try to fit them both with 3 rates for the beta's exponential prior and 3 shape and rate combinations for the cutpoints' gamma prior, a total of 27 model runs.

```{r GammaRecapture, fig.height="200px", cache=TRUE}
# Parallel code actually executes in parallel when running this chuck as a chunck with ctrl+shift+enter. It does not if you run it line by line. I don't know why. 

# format parframe so it works with parLapply better
parlist_gam <- split(parframe_gam, seq(nrow(parframe_gam))) 
names(parlist_gam) <- parframe_gam$modelid

fit_gamma_model <- function(simdatlist, pars, groups) {
  #choose whether to use data simulated with a rapid or slow transition
  if (pars$transition == "slow") {
    simdat <- simdatlist$slow
  }
  if (pars$transition == "medium") {
    simdat <- simdatlist$medium
  }
  if (pars$transition == "fast") {
    simdat <- simdatlist$fast
  }
  #extract parameters for prior distribtuions
  simdat$shape <- pars$shape
  simdat$beta_rate <- pars$beta_rate
  simdat$cut_rate <- pars$cut_rate
  
  #fit the model
  if (isTRUE(groups)) {
    fitgam <- stan(file='dirichlet prior/gamma/gamma_covar_group.stan', data=simdat, chains=4, cores=4)
  } else {
    fitgam <- stan(file='dirichlet prior/gamma/gamma_covar.stan', data=simdat, chains=4, cores=4)
  }
  return(fitgam)
}

# run all models, parallelized

# make a cluster, leaving 20 cores free for other folks
no_cores <- parallel::detectCores() - 20
cl <- parallel::makeCluster(no_cores)

# export the stuff you need to run on the cluster
parallel::clusterExport(cl, c("fit_gamma_model", "parlist_gam", "simdat"))
parallel::clusterEvalQ(cl, c(library(rstan), library(StanHeaders)))

# fit the models
gammafits <- parallel::parLapply(cl, parlist_gam, function(x) {fit_gamma_model(simdatlist = simdat, pars=x, groups=FALSE)})

parallel::stopCluster(cl) #close the cluster

# extract params
paramsgam <- lapply(gammafits,
                    function(x) {data.frame(rstan::extract(x) ) } )
#param summary
sum_gam <- lapply(paramsgam, summary)

#bind true and model pars even tho it will make a giant df

paramsgam <- map(paramsgam, label_names, label="model")
parlist_gam <- map(parlist_gam, label_names, label="true")

paramsgam <- map2(paramsgam, parlist_gam, cbind)
```

```{r gammaParPlots}
#plot params and diffs
#map(paramsgam, parplot)
map(paramsgam, posterior_differencer) %>%
    map2(.y=paramsgam, .f=diffplotter)
```

```{r GammaHDPI}
# calculate whether true value is in 50% hpdi

# kludge HPDI
HPDIlow <- function(x, prob) {
    HPDI <- rethinking::HPDI(x, prob=prob)
    return(HPDI[1])
}

HPDIhigh <- function(x, prob) {
    HPDI <- rethinking::HPDI(x, prob=prob)
    return(HPDI[2])
}


calc_HPDI <- function(params, prob) {
    low <- params %>% dplyr::summarise_at(vars(ends_with("model")), HPDIlow, prob=prob)
    high <- params %>% dplyr::summarise_at(vars(ends_with("model")), HPDIhigh, prob=prob)

    # awkward formatting
    hdpis <- dplyr::full_join(low, high) %>%
        select(-contains("lp"))
    colnames(hdpis) <- stringr::str_replace(colnames(hdpis), "_model", "")

    # true param
    true <- params %>% dplyr::summarise_at(vars(ends_with("true")), unique)
    colnames(true) <- stringr::str_replace(colnames(true), "_true", "")
    true <- select(true, colnames(hdpis))

    # more awkward formatting
    compframe <- dplyr::full_join(hdpis, true) %>%
        t(.) %>%
        data.frame()
    colnames(compframe) <- c("low", "high", "true")
    compframe$params <- rownames(compframe)

    # true param in interval?
    tf <- compframe %>% mutate(inint = true > low & true < high)
    return(tf)

}

in50 <- map(paramsgam, calc_HPDI, prob=0.5)

in75 <- map(paramsgam, calc_HPDI, prob=0.75)

in90 <- map(paramsgam, calc_HPDI, prob=0.90)

# recaptured parameters
perform50 <- map_df(in50, bind_rows, .id="modelid") %>%
  dplyr::mutate(modelid=as.integer(modelid)) %>%
  full_join(parframe_gam) 
perform90 <- map_df(in90, bind_rows, .id = "modelid") %>%
  dplyr::mutate(modelid=as.integer(modelid)) %>%
  full_join(parframe_gam)

# proportion recaptured parameters
prop_recaptured50 <- perform50 %>%
  group_by(modelid) %>%
  summarise(all_captured = sum(inint)/n()) %>%
  full_join(parframe_gam) %>%
  arrange(beta, desc(all_captured))

prop_recaptured90 <- perform90 %>%
  group_by(modelid) %>%
  summarise(all_captured = sum(inint)/n()) %>%
  full_join(parframe_gam) %>%
  arrange(beta, desc(all_captured))

knitr::kable(prop_recaptured50, caption = "Proportion of params recaptured in 50% HPDI")
knitr::kable(prop_recaptured90, caption = "Proportion of params recaptured in 90% HPDI")
```

```{r plotGammaRecaptureoverall}
ggplot(prop_recaptured50, aes(x=as.factor(shape), y=as.factor(beta_rate), fill=all_captured)) + 
  geom_tile(colour="white") +
  geom_text(aes(label=paste(cut_rate, beta_rate, sep=","))) +
  facet_wrap("transition", scales="free") +
  ggtitle("Parameters recaptured in 50% HPDI") +
  scale_fill_gradient(low="white", high="steelblue")

ggplot(prop_recaptured90, aes(x=gammaid, y=as.factor(beta_rate), fill=all_captured)) + 
  geom_tile(colour="white") +
  geom_text(aes(label=paste(cut_rate, beta_rate, sep=","))) +
  facet_wrap("transition", scales="free") +
  ggtitle("Parameters recaptured in 90% HPDI") +
  scale_fill_gradient(low="white", high="violetred")
```
Parameters are most likely to be recaptured when transitions are medium fast (beta=1), shape parameters are higher than cutpoints, and beta is not too constrained (low beta rate). However when transitions are fast (and cutpoints correspondingly larger, very high shape values do not work well.) Choosing shape values that center the gamma prior relatively near the second cutpoint seems to work best.

This is tricky to do, however, without knowing beta - or having a lot of knowledge of your system.
```{r plotGammaRecapture}
knitr::kable(perform50, caption = "Ability of the different models to recapture parameters in 50% HPDI. Each model only run only once, don't shoot me.")

ggplot(perform50, aes(x=params, y=as.factor(modelid), fill=inint)) + 
  geom_tile(color="black") +
  scale_fill_viridis_d()  +
  geom_text(aes(label=paste(shape, beta_rate, sep=","))) +
  facet_wrap("beta", scales="free") +
  ggtitle("Parameters recaptured in 50% HPDI")
    


knitr::kable(perform90, caption = "Ability of the different models to recapture parameters in 90% HPDI. Each model only run only once, don't shoot me.")

ggplot(perform90, aes(x=params, y=as.factor(modelid), fill=inint)) + 
  geom_tile(color="black") +
  scale_fill_viridis_d()  +
  geom_text(aes(label=paste(shape, beta_rate, sep=","))) +
  facet_wrap("beta", scales="free") +
  ggtitle("Parameters recaptured in 90% HPDI")

```



#### Slow transition
Estimates are better for slow parameters when shape parameters are large. When shape parameters are lower, beta and cutpoints are estimated more poorly, but the relationship between beta and cutpoints (the transitions h1 and h2) are still captured well.

#### Fast transition
Estimates are better for fast transitions when choosing a shape parameter near the cutpoints (20).

The proportion of parameters recaptured (in the 50% HDPI) is highest when the transitions are slow (beta=0.5) and the shape parameter for the gamma prior is low. 

```{r proportioncapture}


```
When transitions are slow, large shapes for gamma make it difficult to recapture parameters. When transitions are fast, constraining beta too much (eg. beta rate = 3) harms model performance. Moderate gamma shape parameters work best. 

Getting the shape parameter "right" is more important than the beta rate parameter (over the ranges I've considered anyway). Unfortunately, "right" depends on the speed of transition, which we .... don't know in advance. Given the realities of phenology, I think that it's fair to say the speed is > 1 (at least with forcing units/x scaled the way they are.)

Sadly, slow transition models are easier to fit. 

```{r transitioncapture}
transition_recaptured <- perform90 %>%
  filter(params %in% c("c.1", "c.2")) %>%
  group_by(modelid) %>%
  summarise(h_captured = sum(inint)/n()) %>%
  full_join(parframe_gam) %>%
  arrange(beta, desc(h_captured))
 
knitr::kable(transition_recaptured)
```
The parameters I'm most interested in are the transition parameters (calculated as c/beta), h1 and h2. Unfortunately, these seem relatively difficult to recapture well. In the 50% HDPI, no rapid transition model gets either of these right. The slow model can get them right with a low shape and a constrained beta. In the 90% HDPI, models with rapid transition datasets *almost* catch up to the slow transitions. 

Truly unfortunately, the prior parameters (gamma shape) that make recapture possible are basically opposite for slow and fast transition data. Slow transitions require small to moderate shapes to recapture parameters, but fast transitions require moderate to high transitions to fit. Fast transitions also need a less constrained beta.

Shape = 10 was large for slow transitions and small for fast transitions and made it impossible for both models to recapture true params. 

So how might I go about choosing a shape parameter for a gamma prior? I think I would assume my transition speed (beta) must be >1 and is probably ~2 because of biology. So then, a good gamma shape param choice would probably be around the values your data starts transitioning from state 2 to 3. This feels yucky.


### Induced dirichlet prior with covariate
How about an induced dirichlet prior?

```{r paramsForIDModels}
beta_rate <- c(1:3) # rate parameters for exponential prior on beta

# different anchor points for induced dirichlet

anchors <- c(0, 5, 10)


slowframe_indi <- data.frame(beta = beta_slow, anchor=anchors, beta_rate=beta_rate, c.1 = cutpoints_slow[1], c.2=cutpoints_slow[2], h1=h[1], h2=h[2])
fastframe_indi <- data.frame(beta = beta_fast, anchor=anchors, beta_rate=beta_rate, c.1 = cutpoints_fast[1], c.2=cutpoints_fast[2], h1=h[1], h2=h[2])
parframe_indi <- rbind(slowframe_indi, fastframe_indi) %>%
  tidyr::expand(tidyr::nesting(beta, c.1, c.2, anchor), h1, h2, beta_rate)
parframe_indi$modelid <- 1:nrow(parframe_indi) #label the models
parframe_indi <- dplyr::select(parframe_indi, modelid, beta, c.1, c.2, h1, h2, anchor, beta_rate)

# START HERE
knitr::kable(parframe, caption="beta parameters used to simulate data for a slow (0.5) and fast (2) transion. And shape and rate parameters to be used in the priors on beta and cutpoints to attempt to recover parameters")
```{r dirichlet recapture}
fitdir <- stan(file='dirichlet prior/dirichlet/dirichlet_covar.stan', data=input_data_for_model, chains=4)

paramsdir <- data.frame(rstan::extract(fitdir))
summary(paramsdir)

pdir <- parplotter("dirichlet", paramsdir)
pdir

```
Induced dirichlet does well - maybe better than gamma? It underestimates cutpoints and beta. It's also *very* sensitive to the prior on $\beta$. It fits at exp(1), but the model is full of divergences at exp(2). Gamma is much more forgiving of the choice of $\beta$ prior.

I wonder if it's because of weird stuff happening when both $c$ and $\beta$ get too small? Increasing the anchor point in the induced dirichlet prior to 2 fixes the problem!


```{r compare gamma and dirichlet}


ddir <- diffplotter(paramsdir, priorgroups="dirichlet prior, no groups")
dgam <- diffplotter(paramsgam, priorgroups = "gamma prior, no groups")
ggarrange(dgam, ddir, ncol=1)

ggarrange(pgam, pdir, ncol=1)

```

Dirichlet and gamma performing similarly. Dirichlet a little better maybe? Gamma tends to overestimate parameters and dirichlet to underestimate them.

<!--
## Simulate more data - uniform covariate?
What if your data is, unlike mine, collected more uniformly across the covariate range?

```{r data simulation}
N = 500
G = 7
K = 3

c <- c(3,7) # cutpoints
beta = 0.5
h <- c/beta
x <- runif(N, min=0, max=20) #covariate
hist(x)
```

```{r simulate covar}
input_data_for_simulation <- list("N" = N, "K"=3, "x" = x, "c" = c, "beta" = beta)

simu <- stan(file='dirichlet prior/covar_sim.stan', iter=1, chains=1,
             algorithm="Fixed_param", data=input_data_for_simulation)

simu_params <- rstan::extract(simu)

input_data_for_model <- list("N" = N, "K" = K, "x" = x, "y" = array(simu_params$y[1,]))

inputdf <- data.frame(input_data_for_model)

ggplot(inputdf, aes(x=x, colour=as.factor(y))) +
    stat_ecdf() +
    geom_vline(xintercept=h)

plot(input_data_for_model$x, input_data_for_model$y)
title("Simulated data with cutpoints")
abline(v=h)

```

### Induced dirichlet prior with uniformly distributed covariate
```{r dirichlet recapture}
fitdir <- stan(file='dirichlet prior/dirichlet/dirichlet_covar.stan', data=input_data_for_model, chains=4)

paramsdir <- data.frame(rstan::extract(fitdir))
summary(paramsdir)

pdir <- parplotter("dirichlet", paramsdir)
pdir

diffplotter(model_params=paramsdir, priorgroups="induced dirichlet prior, no groups")

```

When the covariate is more evenly distributed, the dirichlet does a great job! 
-->
## Simulate more data - $\beta=2$

How do the different priors behave when the transitions are sharper, **i.e.** $\beta$ is larger


```{r data simulation}
N = 500
G = 7
K = 3

c <- c(12,28) # cutpoints
beta = 2
h <- c/beta
x <- rtnorm(n=N, mean=mean(h), sd=mean(h)/4, min=0, max=20) #covariate
hist(x)
```

```{r simulate covar}
input_data_for_simulation <- list("N" = N, "K"=3, "x" = x, "c" = c, "beta" = beta)

simu <- stan(file='dirichlet prior/covar_sim.stan', iter=1, chains=1,
             algorithm="Fixed_param", data=input_data_for_simulation)

simu_params <- rstan::extract(simu)

input_data_for_model <- list("N" = N, "K" = K, "x" = x, "y" = array(simu_params$y[1,]))

inputdf <- data.frame(input_data_for_model)

ggplot(inputdf, aes(x=x, colour=as.factor(y))) +
    stat_ecdf() +
    geom_vline(xintercept=h)

plot(input_data_for_model$x, input_data_for_model$y)
title("Simulated data with cutpoints")
abline(v=h)

```



Will the gamma still work well under these conditions?
### Gamma prior with covariate
```{r gamma recapture}
fitgam <- stan(file='dirichlet prior/gamma/gamma_covar.stan', data=input_data_for_model, chains=4)

paramsgam <- data.frame(rstan::extract(fitgam)) 

summary(paramsgam)

pgam <- parplotter("gamma", paramsgam)
pgam

diffplotter(model_params=paramsgam, priorgroups="gamma prior, no groups")
```

Gamma model is relatively sensitive to gamma prior on the cutpoints. When gamma "mean" is the center of the true cutpoints, cutpoints are slightly overestimated, as is beta. h1 is overestimated and h2 is underestimated. When it's 10 (half the "mean" cutpoint here), cutpoints are underestimated by ~2-4). However, the low prior mean with underestimated cutpoints actually estimates h1 and h2 better.

Transitions are estimated better even as the parameters that calculate that transition are estimated worse. So. The "bad" priors are doing a better job at capturing the *relationship* between beta and the cutpoints, which is ultimately what I'm interested in.

On the other hand, I really do need to do a good job estimating beta because otherwise I won't be able to estimate my group effects later on.

### Dirichlet prior with covariate



```{r dirichlet recapture}
fitdir <- stan(file='dirichlet prior/dirichlet/dirichlet_covar.stan', data=input_data_for_model, chains=4)

paramsdir <- data.frame(rstan::extract(fitdir))
summary(paramsdir)

pdir <- parplotter("dirichlet", paramsdir)
pdir

diffplotter(model_params=paramsdir, priorgroups="induced dirichlet prior, no groups")

```



When the transition between states is relatively rapid, the dirichlet struggles a bit. It's really underestimating both cutpoints and beta - but getting the inflection points quite right. This occurs with a $\beta$ of exp(2) and 1. If the $\beta$ prior is too loose (exp(0.5)) or too tight (exp(4)), the induced dirichlet fails.

The anchor point in the dirichlet also really matters to cutpoint estimation. Anchors much lower than the first true cutpoint really push the cutpoint estimate down. Anchors closer to the midpoint of the true cutpoint do better at estimating cutpoints, but worse at estimating transitions.





Cut point anchor medium

```{r exponential}
hist(rexp(1000,0.5), breaks=30)
hist(rexp(1000,2), breaks=30)
hist(rexp(1000,4), breaks=30)
```

With rapid transitions, the model with the induced dirichlet can do ok at estimating the inflection points, but if you aren't making good choices about $\beta$ it will fail hard.

What if the anchor point is closer to the first transition? START HERE maybe already done too tired.

```{r compare gamma and dirichlet}


ddir <- diffplotter(paramsdir, priorgroups="dirichlet prior, no groups")
dgam <- diffplotter(paramsgam, priorgroups = "gamma prior, no groups")
ggarrange(dgam, ddir, ncol=1)

ggarrange(pgam, pdir, ncol=1)

```

Gamma prior model does underestimate both cutpoints and beta, but not as much as the dirichlet does. However, the inflection points/half transition points (h1 and h2) are maybe estimated a bit better by the dirichlet?

## Conclusions on gamma vs induced dirichlet
For my case - where cutpoints and beta are positive and most data is likely to be collected around state 2.




# Covariate and a group

What if our data has groups that have different effects? 

## Group effects just a little smaller than $\beta$ 
What if the effects are around the same size as beta and beta is relatively small (which makes the model harder to fit)?

Let's simulate 7 groups with `N=500` observations for each group. Effects for the groups are simulated from a normal distribution with `mean=0` and `sd=0.25`. They're deviations from the population mean.

```{r group simulation}

N = 500*7
G = 7
K = 3

# parameters
c <- c(3,7) # cutpoints
beta = 0.5

# simulate individual group effects
gbeta_mu <- 0
gbeta_sd <- 0.25

gbeta_vec <- rnorm(G, mean=gbeta_mu, sd=gbeta_sd) # group effects
gbeta <- sort(sample(gbeta_vec, size=N, replace=TRUE)) # assign a group effect to every observation
gid <- as.numeric(as.factor(gbeta)) # label the groups
groupeffects <- data.frame(gbeta, gid) %>%
  unique()

h1 <- c[1]/(beta+groupeffects$gbeta)
h2 <- c[2]/(beta+groupeffects$gbeta)
x <- rtnorm(n=N, mean=mean(c(h1,h2)), sd=mean(h)/4, min=0, max=20) #covariate centered around transitions
hist(x)

input_data_for_simulation <- list("N" = N, "K"=K, "G"=G, "x" = x, "c" = c, "beta"=beta, "gbeta"= gbeta)

simu <- stan(file='dirichlet prior/covar_group_sim.stan', iter=1, chains=1,
             algorithm="Fixed_param", data=input_data_for_simulation)

simu_params <- rstan::extract(simu)

input_data_for_model <- list("N" = N, "K" = K, "G"=G, "x" = x, "GID"= gid, "y" = array(simu_params$y[1,]))

table(input_data_for_model$y)
plot(input_data_for_model$x, input_data_for_model$y)

groupdata <- data.frame(x=x, y=input_data_for_model$y, GID=gid)
ggplot(groupdata, aes(x=x, color=as.factor(y))) +
    stat_ecdf() +
    facet_wrap("GID")
```

### Gamma recapture 
Can a model with the gamma prior recapture the parameters used to simulate the data?

The group effect prior ~ $\mathrm{normal}(0,sigma_{group})$ with $sigma_{group} ~ \mathrm{exponential}(4)$

```{r gamma recapture group}
fitgam <- stan(file='dirichlet prior/gamma/gamma_covar_group.stan', data=input_data_for_model, chains=4, control = list(adapt_delta=0.95, max_treedepth=11), iter=5000)

paramsgamg <- data.frame(rstan::extract(fitgam)) 

summary(paramsgamg)

# function to make a graph for a group of parameter values comparing them to the "true" value


parplot <- function(label, pardf) {
  cutpointsp <- mcmc_areas(pardf, regex_pars="c") +
    geom_vline(xintercept=c)
  betap <- mcmc_areas(pardf, pars="beta") +
    geom_vline(xintercept=beta)
  betagp <- mcmc_areas(pardf, regex_pars="betag") +
    geom_vline(xintercept = groupeffects$gbeta)
  hp <- mcmc_areas(pardf, regex_pars = "fstart") +
    geom_vline(xintercept=h1)
  ggpubr::ggarrange(cutpointsp, betap, betagp, hp, labels=label)
}

pgam <- parplot("gamma, beta=0.5", paramsgamg)
pgam

diffplotter(model_params=paramsgam, priorgroups="gamma prior, no groups")
```

### These notes are for data simulated around base population h (with group effects subtracted out), not individual group h's.
There was a divergent transition, treedepth warning, and ESS warnings with the default adapt delta, so I increased it to 0.9. Still problems. 

Trying 0.95. No more divergences, but 1 transition after warmup exceeded max treedept and still some bulk ESS problems. $\beta$ parameters are all overestimated.

Keeping adapt_delta at 0.95 and increasing max treedepth to 11 leaves only ESS problems. Estimates are not great - all beta parameters are overestimated and there's no differentiation between them. Not ideal.

Attempting an increase in iterations - that kills all the Stan errors! Unfortunately, $\beta$, $\beta_g$s, and $sigma_group$ are still really overestimated and broad (just bad!). And none of the $\beta_g$s are negative.

On the one hand, this is not great from a model perspective. OTOH, these are pretty unrealistic group effects - ones that shift fstart quite a lot.

### These notes are for data simulated around an h based on the entire population, including groups

Tail ESS errors with iter at 4000, increased to 5000. No more warnings. But how does estimation go? Still badly. Fails to distinguish between groups, overestimates beta, underestimates cutpoints.


## Group effects about 1/4 the size of beta.
Is it the relative size of $beta$ and $betag$s that's the problem or the absolute size of $betag$? Let's bump up $beta$ to the easier to fit 2 (more rapid state transitions) and see how our model does.

```{r group simulation big beta}

N = 500*7
G = 7
K = 3

# parameters
c <- c(5,10) # cutpoints
beta = 2

# simulate individual group effects
gbeta_mu <- 0
gbeta_sd <- 0.25

gbeta_vec <- rnorm(G, mean=gbeta_mu, sd=gbeta_sd) # group effects
gbeta <- sort(sample(gbeta_vec, size=N, replace=TRUE)) # assign a group effect to every observation
gid <- as.numeric(as.factor(gbeta)) # label the groups
groupeffects <- data.frame(gbeta, gid) %>%
  unique()

h1 <- c[1]/(beta+groupeffects$gbeta)
h2 <- c[2]/(beta+groupeffects$gbeta)
x <- rtnorm(n=N, mean=mean(c(h1,h2)), sd=mean(h), min=0, max=20) #covariate
hist(x, breaks=30)
     

input_data_for_simulation <- list("N" = N, "K"=K, "G"=G, "x" = x, "c" = c, "beta"=beta, "gbeta"= gbeta)

simu <- stan(file='dirichlet prior/covar_group_sim.stan', iter=1, chains=1,
             algorithm="Fixed_param", data=input_data_for_simulation)

simu_params <- rstan::extract(simu)

input_data_for_model <- list("N" = N, "K" = K, "G"=G, "x" = x, "GID"= gid, "y" = array(simu_params$y[1,]))

table(input_data_for_model$y)
plot(input_data_for_model$x, input_data_for_model$y)

groupdata <- data.frame(x=x, y=input_data_for_model$y, GID=gid)
ggplot(groupdata, aes(x=x, color=as.factor(y))) +
    stat_ecdf() +
    facet_wrap("GID")
```


```{r gamma recapture group big beta}
fitgam <- stan(file='dirichlet prior/gamma/gamma_covar_group.stan', data=input_data_for_model, chains=4, control = list(adapt_delta=0.95, max_treedepth=11), iter=4000)

fitgam <- stan(file='dirichlet prior/gamma/gamma_covar_group.stan', data=input_data_for_model, chains=4)

paramsgamg <- data.frame(rstan::extract(fitgam)) 

summary(paramsgamg)

# function to make a graph for a group of parameter values comparing them to the "true" value


pgam <- parplot("gamma, beta=2", paramsgamg)
pgam

```

Model doesn't give any obvious warnings, but  it overestimates cutpoints, underestimates beta, and dramatically overestimates betags.

If I change the priors from `cutpoints ~ gamma(10,1)` to `gamma(5,1)` and reduce the sigma_group prior to `exp(5)` from `exp(4)`, it does the same.

If I bump up cutpoints so that the covariate distribution isn't bumping up against 0, beta is underestimated and group betas really overestimated.

If I strongly constrain the sigma_group prior to `exp(10)`, it doesn't change anything.

If I use a uniform distribution for the covariate, beta is underestimated, group betas are still really overestimated (maybe even worse!) and not differentiated.

Why are all the group effect betas positive and undifferentiated? Does the model need more data? This run took... a really long time. 44109.6 seconds. And it had warnings... And the fit was even worse.

```
Warning messages:
1: There were 393 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 11. See
http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded 
2: Examine the pairs() plot to diagnose sampling problems
```

Can the group effects be recovered if I explicity specify the sd as 0.25 in the model? No it cannot. That's depressing.

What if beta is bigger compared to group effects - like 5? Even more disastrous. Beta is SO overestimated.

What if I drop beta to 1? Still garbage with betas all so overestimated.

I think I might be simulating my data kind of poorly - or maybe this model isn't good about simulating this data? Or maybe beta should be bigger? I think the transitions between states occurs faster in my data.

I could also try a really big beta - like 10.
