---
title: "Choosing priors in an ordinal logistic model"
author: "Susannah Tysor"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: html_document
---

```{r setup}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message=FALSE)
```

```{r packages}
library(dplyr)
library(ggplot2)
library(bayesplot)
library(rstan)
library(cowplot)
library(testthat)
library(parallel)
library(purrr)
```

```{r depends}
source('prior_analysis_functions.R')
```

```{r options}
rstan_options(auto_write=TRUE)
```

```{r global}
# each version of a model should be run how many times?
reps <- 30
```


Michael Betancourt thinks that an [induced dirichlet prior](https://betanalpha.github.io/assets/case_studies/ordinal_regression.html) on the cut points may be a good choice for ordinal logistic models.

When I modelled phenology using an ordinal logistic model, I used

* a gamma distribution for the prior on the cut points, $c$
* an exponential distribution distribution for the transition speed/slope, $\beta$
    * normal distributions for group effects on the slope

Was this even a good idea?

Normally, priors are bottom up - you choose a prior distribution for each parameter in your model. This is tricky for the cut points in an ordinal logistic model because they're defined on an abstract latent space where your domain expertise doesn't really apply. But you really want to choose a good prior in ordinal logistic models, because ordinal logistic models with covariates are inherently non-identifiable. (Because $\beta$ and $c$ depend on one another.)

Betancourt says that

> To avoid the non-identifiability of the interior cut points from propagating to the posterior distribution we need a principled prior model that can exploit domain expertise to consistently regularize all of the internal cut points at the same time. Regularization of the cut points, however, is subtle given their ordering constraint. Moreover domain expertise is awkward to apply on the abstract latent space where the internal cut points are defined.

He develops an induced dirichlet prior on the cut points. I want to compare the induced dirichlet and gamma priors on cut points for my situation. 

If I simulate data like mine using an ordinal logistic model, can I recapture the parameters using an ordinal logistic model with a gamma or induced dirichlet prior on the cut points?

At this point, I don't actually care about recapturing the cut points or beta. I do care about recapturing the transition points, $h$, though. $h$ are the points at which half of the population has transitioned from one state to another and is calculated as $h = \frac{c}{\beta}$. Eventually I will want to consider how different groups affect $h$ through deviations from the main slope $\beta$ (*e.g* $h = \frac{c}{\beta + \beta_{group_i}}$) and so estimating $\beta$s well might be more important then. But for now, let's just consider a an ordinal logistic model with a covariate/latent effect and no groups.

## Goals
1) When can I recapture parameters with a gamma prior?
2) When can I recapture parameters with an induced dirichlet prior?
3) Which one works best?

# Simulate data
First let's simulate some data that's kind of like mine.

I have three potential states $K$ and a latent effect/covariate $x$. $x$ is always positive and measurements are only made when $x$ is between `8` and `18`. Data collection begins when workers notice trees are approaching state `2` and stops as trees enter state `3`.

I simulate 3 datasets with slow ($\beta = 0.5$), medium ($\beta = 1$), and fast ($\beta = 2$) transition speeds. I want the transitions to occur at the same $x$ for both datasets. The halfway transition points $h$ will be at `x= 10` and `x=15`. $h$ is calculated with cut points $c$ and transition speed $\beta$, that is $h = \frac{c}{\beta}$ So while the transition points will stay the same in each simulated dataset, the cut points will differ. 

Cut points for the slow transition dataset will be at 5 and 7.5, at 10 and 15 for the medium and at 20 and 30 for the fast transition.

Here is what the first transition between states looks like for those transitions. My data is more like the fast transition - a phenological period with transitions of beta=0.5 would be occuring over an unrealistically long time period.

```{r transitions, fig.height=3, fig.width=4}

logistic2 <- function(x, beta, c) {
  y <- 1/(1+exp(-beta*x + c))
}

x <- c(0:20)
logisticexp <- data.frame(x, slow = logistic2(x, 0.5, 5), fast=logistic2(x, 2, 20), medium=logistic2(x, 1,10)) %>%
  tidyr::pivot_longer(cols=c(slow, medium, fast))

ggplot(logisticexp, aes(x=x, y=value, color=name)) +
  geom_line()
```

Now the real fake data:

```{r simulate_data}
simulation_input <- set_simulation_parameters(N=500)

simdat <- map(simulation_input$inputlist, simulate_data, groups=FALSE)

plot_simulated_data(simdat, simulation_input$inputlist)

```

## Attempt to recapture parameters with a gamma prior on cutpoints

Now I want to try to recapture parameters.

I need to know how good I have to be at choosing the shape and rate parameters for the gamma prior on the cut points to recapture the "true" slope and cut points parameters as well. (The mean of the gamma is the shape divided by the rate.)  

Choosing where to center the gamma isn't obvious since cut points are on that "abstract latent space." Since $c = \beta h$, though, we have some idea of what that is. $x=12.5$ is smack dab in the middle of our transition points $h$, which we can roughly estimate from the data. It would be nice to be able to use this number that we know when choosing the parameters for the gamma prior. 

If we center a gamma distribution on state `2`, how much wiggle room does it need to find the cut points? When $\beta=0.5$, `12.5` is `6.25` on the cut point space and when $\beta=2$, `12.5` is `25` on the abstract latent space. The larger $\beta$ is, the bigger and further apart the cut points are. The smaller $\beta$ is, the smaller and closer together the cut points are. Since we don't know $\beta$ in advance, we probably have to choose a fat gamma so we can access the full range of values where cut points could be. On the other hand, we might end up estimating combinations of $c$ and $\beta$ that get $h$ right, but both $c$ and $\beta$ very wrong.

I also want to understand how the exponential prior on $\beta$ affects the ability of the model to recapture parameters. I'll try rates of 1,2,3.


```{r paramsForGammaModels}

beta_rate <- c(1:3) # rate parameters for exponential prior on beta

# shape and rate parameters for gamma prior on cut points, all centered on mean transition point

factors <- list(fat=0.10, middling=0.25, skinny=1)

pars_gamma <- purrr::map(factors, make_gamma_pars, h=simulation_input$h)  

#plot gamma priors
pars_gamma %>%
  # format for plotting
  map(function(x) {rgamma(1000, x$shape, x$cut_rate)}) %>%
  bind_cols() %>%
  tidyr::pivot_longer(cols = names(factors)) %>%
  # plot
  ggplot(aes(x=value, fill=name)) +
  geom_density(alpha=0.5) +
 # facet_grid(name ~ .) +
  ggtitle("Gamma priors all centered on 10", subtitle = "with different shape and rate params")

# make a nice dataframe with all combinations params used to simulate data and model params used to try to recover those params
parframe_gam <- pars_gamma %>%
  purrr::map_dfr(.f=bind_cols, .id=".id") %>%
  merge(beta_rate) %>%
  dplyr::rename(gammaid=.id, beta_rate=y) %>%
  merge(simulation_input$pars)
parframe_gam$h1 <- simulation_input$h[1]
parframe_gam$h2 <- simulation_input$h[2]
parframe_gam$modelid <- 1:nrow(parframe_gam) #label the models

# format parframe so it works with parLapply better
parlist_gam <- make_parframe_list(parframe_gam)


```

So there are 3 simulated datasets - 1 for each of three transition speeds. I'm going to try to fit them both with 3 rates for the $\beta$'s exponential prior and 3 shape and rate combinations for the cut points' gamma prior, a total of 27 model runs. 

```{r paramsTableGamma}
knitr::kable(parframe_gam, caption="Model parameterizations used to try to recapture params - gamma")
```


```{r GammaRecapture, include=FALSE, warning=TRUE, }
# # Parallel code actually executes in parallel when running this chuck as a chunck with ctrl+shift+enter. It does not if you run it line by line. I don't know why. 
# 
# # run all models, parallelized
# 
# # make a cluster using half your cores
# no_cores <- parallel::detectCores()/2
# cl <- parallel::makeCluster(no_cores)
# 
# # export the stuff you need to run on the cluster
# parallel::clusterExport(cl, c("fit_gamma_model", "parlist_gam", "simdat"))
# parallel::clusterEvalQ(cl, c(library(rstan), library(StanHeaders)))
# 
# 
# for (i in 1:reps) {
#   fits_gam <- parallel::parLapply(cl, parlist_gam, function(x) {fit_gamma_model(simdatlist = simdat, pars=x, groups=FALSE)})
#  # fits_gam_list[[i]] <- fits_gam
#  saveRDS(fits_gam, file = paste0("gamma/covar_runs/run", i, ".rds"))
#  rm(fits_gam) # how much ram do you think you have? Don't crash everything.
#  gc() # clean your room 
# }
# 
# parallel::stopCluster(cl) #close the cluster

```

```{r readGammaModels, warning=FALSE, cache=TRUE}

# pull in parameters and info on divergences, etc from saved stanfit objects

extracts_gam <- extract_pars_and_problems(path="gamma/covar_runs/", parlist=parlist_gam)
```
### Model check
Do a naive check for model problems - divergences and issues with rhats and neff_ratios.

```{r gammaModelCheck}
problems_gam <- extracts_gam$problems %>%
  map_dfr(bind_rows, .id=".id") %>%
  rename(run=.id) %>%
  group_by(modelid) %>%
  summarize(bad_proportion = n()/reps, bad_count = n(), divergences_mean=mean(divergences), bad_rhats_mean=mean(bad_rhats), bad_neff_mean=mean(bad_neff))
  
knitr::kable(problems_gam, caption = "Models with divergences, rhats > 1, or neff_ratios < 0.1 - gamma")

```
18/27 models had problems with Rhat or neff_ratio in 1-5 of the 30 model runs. No divergences ever though. That's not ideal

[ADD A FILTER HERE SHOWING THE PARAM VALUES THAT DON'T CAUSE PROBLEMS]

```{r gammaPrettyModels}

```

```{r gammaExtract}

# extract parameters from stan objects and pair with true param values
#params_gam <- bind_true_model_pars(fits_gam, parlist_gam)
params_gam <- extracts_gam$pars %>%
  map_dfr(bind_rows, .id = ".id") %>%
  rename(run=.id) %>%
  split(.$modelid)

```

### Recapture rate

Of the 5 parameters ($\beta$, $c$ and derived $h$), how many are recaptured by the model?
```{r GammaHDPI, message=FALSE, warning=FALSE, cache=TRUE}
# calculate whether true values are in HPDI

recaptured_gam <- which_params_recaptured(params_gam) # in or out
prop_recaptured_gam <- calc_prop_recaptured_overall(recaptured_gam, parframe_gam) # average number of parameters recaptured by each model
prop_recaptured_by_param_gam <- calc_recaptured_by_param(recaptured_gam, parframe_gam) #proportion of model runs where a parameter is returned correctly


```

```{r plotGammaRecaptureoverall, caption="Number of parameters recaptured by each model, averaged across model runs"}
ggplot(prop_recaptured_gam$fifty, aes(x=gammaid, y=as.factor(beta_rate), fill=mean_captured)) + 
  geom_tile(colour="white") +
  geom_text(aes(label=modelid)) +
  facet_wrap("transition", scales="free") +
  ggtitle("Proportion of parameters recaptured in 50% HPDI", subtitle = "faceted by transition speed in simulated dataset") +
  scale_fill_viridis_c(limits=c(0,5)) +
  ylab("beta_rate")

ggplot(prop_recaptured_gam$ninety, aes(x=gammaid, y=as.factor(beta_rate), fill=mean_captured)) + 
  geom_tile(colour="white") +
  geom_text(aes(label=modelid)) +
  facet_wrap("transition", scales="free") +
  ggtitle("Proportion of parameters recaptured in 90% HPDI",subtitle = "faceted by transition speed in simulated dataset") +
  scale_fill_viridis_c(limits=c(0,5)) +
  ylab("beta_rate")
```

Most models return all parameters in the 90% HPDI, but a skinny gamma prior makes it harder to return parameters, especially when $\beta$ deviates from 1. 

Parameters are recaptured better when transition speed is fast, as long as the cutpoints prior isn't too skinny.

Which parameters are easier or harder to recapture?

```{r plotGammaRecapture, caption="Proportion of model runs where the HDPI contains the true parameter value"}

ggplot(prop_recaptured_by_param_gam$fifty, aes(x=param, y=as.factor(modelid), fill=prop_inint)) + 
  geom_tile(color="black") +
  scale_fill_viridis_c()  +
  geom_text(aes(label=paste(gammaid, beta_rate, sep="\n")), size=2.5) +
  facet_wrap("transition", scales="free") +
  ggtitle("Parameters recaptured in 50% HPDI", subtitle = "faceted by transition speed (beta) in simulated dataset\n with gamma shape and beta rate labels") +
  theme(legend.position = "top") +
  ylab("model id")

ggplot(prop_recaptured_by_param_gam$ninety, aes(x=param, y=as.factor(modelid), fill=prop_inint)) + 
  geom_tile(color="black") +
  scale_fill_viridis_c()  +
  geom_text(aes(label=paste(gammaid, beta_rate, sep="\n")), size=2.5) +
  facet_wrap("transition", scales="free") +
  ggtitle("Parameters recaptured in 90% HPDI", subtitle = "faceted by transition speed (beta) in simulated dataset\n with gamma shape and beta rate labels") +
  theme(legend.position = "top") +
  ylab("model id")

```

$\beta$ and $c$ are never recaptured in the 50% HPDI when the real $\beta$ is slow, but they are usually recaptured in the 90% HPDI, except when the gamma prior on cutpoints is skinny.

A skinny gamma prior centered in between the transition points makes it difficult to recapture $\beta$ and $c$ across the board, unless $\beta$ is near 1 (See models 3,6,9, 21, 24, 27). 

$beta_rate$ choice doesn't seem to matter very much, but when transition speed is fast, a too loose $beta_rate$ makes it harder to recapture $\beta$ and $c$. When transitions are slow, a too tight beta rate might make estimates worse.

#### Half transition points $h$

Both $h$ are always captured in the 90% HPDI, even when $\beta$ and $c$ are not recaptured. So the *ratio* between $\beta$ and $c$ is being captured, even when the actual values of those parameters aren't.

$h_2$ is estimated best (*i.e* in 50% HPDI) when transitions are fast and $h_1$ is estimated best when transitions are slow. 

Good estimates for $\beta$ and $c$ are correlated with worse estimates for $h$. This is a concerning tradeoff.

#### Best prior parameter choices

Fat to middling gamma shape. 

Beta rate doesn't matter that much, but 2 works best.

Assume centered between transitions because that's all I tried. Could also try centering it closer to one transition or the other, or changing skew. 

The best choices for your prior parameters in this situation are a moderately loose $\beta_rate$ of 2 and a fat to middling gamma distribution.

```{r plotGammaParams, fig.height=3, fig.width=4}

map(params_gam[base::sample(1:27, 5)], parplot) # pick 5 models at random to plot

```

## Attempt to recapture parameters with induced dirichlet prior on cutpoints
Let's repeat this analysis using an induced dirichlet prior on cutpoints. I'll use the same simulated data as above.

I'll test the same $beta_{rate}$ parameters as above. I'll also vary the anchor parameter in the induced dirichlet prior.

In his example, Betancourt sets the anchor to 0. I don't completely understand how the anchor works, so I'll try 5 and 10 as well and see what happens.

```{r paramsForInducedDirichletModels}

beta_rate <- c(1:3) # rate parameters for exponential prior on beta
anchor <- c(0, 5, 10) # different anchor parameters for induced dirichlet prior

# make a nice dataframe with all combinations params used to simulate data and model params used to try to recover those params

parframe_indir <- merge(simulation_input$pars, y=beta_rate) %>%
    rename(beta_rate=y) %>%
    merge(y=anchor) %>%
    rename(anchor=y) %>%
    mutate(h1 = simulation_input$h[1], h2 = simulation_input$h[2])
parframe_indir$modelid <- 1:nrow(parframe_indir)

# format parframe so it works with parLapply better
parlist_indir <- make_parframe_list(parframe_indir)
```

I'm going to fit all three simulated datasets with each of the 3 rate prior parameters on beta and anchor parameters on the cutpoints.

``````{r paramsTableIndir}
knitr::kable(parframe_indir, caption="Model parameterizations used to try to recapture params - induced dirichlet")
```

```{r indirRecapture, include=FALSE}


# # run all models, parallelized
# 
# # make a cluster using half your cores
# no_cores <- parallel::detectCores()/2
# cl <- parallel::makeCluster(no_cores)
# 
# # export the stuff you need to run on the cluster
# parallel::clusterExport(cl, c("fit_indir_model", "parlist_indir", "simdat"))
# parallel::clusterEvalQ(cl, c(library(rstan), library(StanHeaders)))
# 
# 
# for (i in 1:reps) {
#   fits_indir <- parallel::parLapply(cl, parlist_indir, function(x) {fit_indir_model(simdatlist = simdat, pars=x, groups=FALSE)})
#   saveRDS(fits_indir, file = paste0("induced_dirichlet/covar_runs/run", i, ".rds"))
#   rm(fits_indir)
#   gc()
# }
# 
# parallel::stopCluster(cl) #close the cluster

```

```{r readIndirModels, warning=FALSE, cache=TRUE}

# pull in parameters and info on divergences, etc from saved stanfit objects
# very slow step. consider parallelizing to the extent your ram can handle

extracts_indir <- extract_pars_and_problems(path="induced_dirichlet/covar_runs/", parlist=parlist_indir)
```
### Model check
Do a naive check for model problems - divergences and issues with rhats and neff_ratios.

```{r indirModelCheck}
problems_indir <- extracts_indir$problems %>%
  map_dfr(bind_rows, .id=".id") %>%
  rename(run=.id) %>%
  group_by(modelid) %>%
  summarize(bad_proportion = n()/reps, bad_count = n(), divergences_mean=mean(divergences), bad_rhats_mean=mean(bad_rhats), bad_neff_mean=mean(bad_neff))
  
knitr::kable(problems_indir, caption = "Models with divergences, rhats > 1, or neff_ratios < 0.1 - induced dirichlet")

```

15/27 models had problems with Rhat or neff_ratio in 1-6 of the model runs. Only model 4 had neff_ratio problems.

```{r indirExtract}

# extract parameters from stan objects and pair with true param values
params_indir <- extracts_indir$pars %>%
  map_dfr(bind_rows, .id = ".id") %>%
  rename(run=.id) %>%
  split(.$modelid)

```

### Recapture rate

Of the 5 parameters ($\beta$, $c$ and derived $h$), how many are recaptured by the model?

```{r indirHDPI, message=FALSE, warning=FALSE, cache=TRUE}
# calculate whether true value is in HPDI

recaptured_indir <- which_params_recaptured(params_indir)
prop_recaptured_indir <- calc_prop_recaptured_overall(recaptured_indir, truepars=parframe_indir)
prop_recaptured_by_param_indir <- calc_recaptured_by_param(recaptured_indir, parframe_indir) 

```

```{r plotIndirRecaptureoverall}
ggplot(prop_recaptured_indir$fifty, aes(x=as.factor(anchor), y=as.factor(beta_rate), fill=mean_captured)) + 
  geom_tile(colour="white") +
  geom_text(aes(label=modelid)) +
  facet_wrap("transition", scales="free") +
  ggtitle("Proportion of parameters recaptured in 50% HPDI", subtitle = "faceted by transition speed in simulated dataset") +
  scale_fill_viridis_c(limits=c(0,5)) +
  ylab("beta_rate")

ggplot(prop_recaptured_indir$ninety, aes(x=as.factor(anchor), y=as.factor(beta_rate), fill=mean_captured)) + 
  geom_tile(colour="white") +
  geom_text(aes(label=modelid)) +
  facet_wrap("transition", scales="free") +
  ggtitle("Proportion of parameters recaptured in 90% HPDI",subtitle = "faceted by transition speed in simulated dataset") +
  scale_fill_viridis_c(limits=c(0,5)) +
  ylab("beta_rate")
```

Using an induced dirichlet prior on the cutpoints doesn't seem to work very well.

Which parameters are easier or harder to recapture?

```{r plotIndirRecapture}

ggplot(prop_recaptured_by_param_indir$fifty, aes(x=param, y=as.factor(modelid), fill=prop_inint)) + 
  geom_tile(color="black") +
  scale_fill_viridis_c()  +
  geom_text(aes(label=paste(anchor, beta_rate, sep="\n")), size=2.5) +
  facet_wrap("transition", scales="free") +
  ggtitle("Parameters recaptured in 50% HPDI", subtitle = "faceted by transition speed (beta) in simulated dataset\n with anchor and beta rate labels") +
  theme(legend.position = "top") +
  ylab("model id")

ggplot(prop_recaptured_by_param_indir$ninety, aes(x=param, y=as.factor(modelid), fill=prop_inint)) + 
  geom_tile(color="black") +
  scale_fill_viridis_c()  +
  geom_text(aes(label=paste(anchor, beta_rate, sep="\n")), size=2.5) +
  facet_wrap("transition", scales="free") +
  ggtitle("Parameters recaptured in 90% HPDI", subtitle = "faceted by transition speed (beta) in simulated dataset\n with anchor and beta rate labels") +
  theme(legend.position = "top") +
  ylab("model id")

```
When transitions are fast, no anchor allows $\beta$ and $c$ to be recaptured, but half transition points $h$ are (but $h_2$ isn't ever captured in 50% HPDI, only 90%). 

When transitions are medium speed, a large anchor allows all parameters to be recaptured in the 90% HPDI. Any anchor allows $h$ to be recaptured in the 90%, but $h_2$, again, is never captured in the 50%.

When transitions are slow, $h2$ is never recaptured, but all other parameters are recaptured in the 90% HPDI. $\beta$ is recaptured better when anchors are lower, $c_1$ is captured better when anchor is 5, and $h_1$ is captured better when anchors are large (10).

## Best prior parameter choices

```{r plotIndirParams, fig.height=3, fig.width=4}

map(params_indir[base::sample(1:27, 5)], parplot) # pick 5 models at random to plot

```
Five random models for induced dirichlet.
