---
title: "Choosing priors in an ordinal logistic model"
subtitle: "with covariate and group effects"
author: "Susannah Tysor"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  html_document:
    theme: flatly
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message=FALSE)
```

```{r}
library(dplyr)
library(rstan)
library(tidyr)
library(purrr)
library(cowplot)
library(bayesplot)
library(ggplot2)

rstan_options(auto_write=TRUE)
```

```{r depends}
source('prior_analysis_groups_functions.R')
```

```{r global}
# each version of a model should be run how many times?
reps <- 5
```

# Background

[Previously](https://scisus.github.io/ordinal_logistic_prior/prior_analysis.html), I considered whether a gamma or an induced dirichlet prior on cut points in an ordinal logistic model worked better for the kind of data I'm considering. Neither was very promising, but because of problems with convergence, etc. in models with the gamma prior, I'm testing group effects with the induced dirichlet.

In this model, the state of flowering depends on how many forcing units have accumulated. 

There are three possible states: 1-not yet flowering, 2- flowering, 3- done flowering. 

$x$ is accumulated forcing: when flowering was observed, how much warmth had the population/tree been exposed to since January 1? $x$ is always positive (and always increases monotonically through time - though time is abstracted out of this model).

$\beta$ describes how fast the transition occurs - small $\beta$s make the transition from between states occur over a wider range of $x$'s. (Translated from forcing units to days, this answers a question like "does the population transition between states over 1 day, 3 days, a week?" We work in forcing unit space because the trees respond to temperature, not time - and no spring heats up exactly the same so dates are kind of useless for prediction.) $\beta$ is always positive.

In [the case without groups](https://scisus.github.io/ordinal_logistic_prior/prior_analysis.html), an ordinal logistic model with an induced dirichlet prior on cutpoints can struggle to recapture $\beta$ and cut points $c$, but it is pretty good at capturing the relationship between $\beta$ and $c$: $h = \frac{c}{\beta}$, which is the point at which half the trees in a population have transitioned or the point at which an individual tree is 50% likely to have transitioned. 

In the previous analysis, no groups were considered. In reality, I want to know if things like site, provenance, and clone affect $h$.

## Goals
1 - Generate data with an ordinal logistic model 
   - With linear model structure $\beta x + \alpha$
2 - Fit model with Stan
3 - Determine whether $\beta$, $\beta_{site_i}$, \alpha$, $c$, or $h$ can be returned.


I'll simulate data for 10 groups with `N=100` observations for each group. Each group's $h$ are shifted by $h_mod$, which is drawn from a $\mathcal{N}(0,\sigma)$. Data is simulated for two transition speeds, $\beta=1$ and $\beta=2$.

First we'll do this with null effects ($\sigma=0$), then with a relatively large range of effects (\sigma = 1).

# Can null group effects be detected?

Simulate data from an ordinal logistic model where there are 10 groups, but none of the groups have an effect.

The Stan model for data simulation is:

```{r}
writeLines(readLines("simulate/covar_group_alpha_sim.stan"))
```
Plots of simulated data for each group should look roughly identical.

```{r nullGroupSimulation, fig.height=11, fig.width=5, fig.cap="Simulated data in each group. Since groups have no effect in test, all groups should look more or less identical.}

G = 10
h_mod0 <- rep(0, G)

simulation_input <- set_simulation_parameters(G=G, hmod=h_mod0)


simdat_alpha <- purrr::map(.x=simulation_input$inputlist, .f=simulate_data)

plot_simulated_data(simdat_alpha, simulation_input$inputlist)
```

## Estimate parameters

Next we'll check whether models can accurately recapture parameters. Based on findings from the no-group investigation, we'll create models with an exponential prior on $\beta$ with a rate of 1, 2, or 3 and the anchor for the dirichlet inducing the priors on cutpoints at 10 and 20. 



```{r params}

beta_rate <- c(1:3) # rate parameters for exponential prior on beta
anchor <- c(10, 20) # different anchor parameters for induced dirichlet prior

# make a nice dataframe with all combinations params used to simulate data and model params used to try to recover those params
# 

parframe_indir <- simulation_input$pars %>% 
  dplyr::select(transition, beta, c.1, c.2, group, alpha_group) %>%
  tidyr::pivot_wider(names_from = group, names_prefix = "alpha_g.", values_from = alpha_group) %>% # put in wide format
  merge(y= beta_rate) %>%
    rename(beta_rate=y) %>%
    merge(y=anchor) %>%
    rename(anchor=y)
parframe_indir$modelid <- 1:nrow(parframe_indir) #label the models

# make a wide format with beta_group cols and merge with parframe_indir

# format parframe so it works with parLapply better
make_parframe_list <- function(parframe) {
    parlist <- split(parframe, seq(nrow(parframe)))
    names(parlist) <- parframe$modelid

    return(parlist)
}
parlist_indir <- make_parframe_list(parframe_indir)
```

```{r indirRecapture, include=FALSE}
# fit a model with an induced dirichlet prior on cutpoints in stan. simdatlist is a list of simulated data (1 simulated dataset per list entry), pars is a list of parameter values (1 set of parameter values per list entry) and groups is TRUE or FALSE indicating whether you're trying to fit groups.
fit_indir_model <- function(simdatlist, pars, modification) {
    #choose whether to use data simulated with a rapid or slow transition
    if (pars$transition == "medium") {
        simdat <- simdatlist$medium
    }
    if (pars$transition == "fast") {
        simdat <- simdatlist$fast
    }
    #extract parameters for prior distributions
    simdat$anchor <- pars$anchor
    simdat$beta_rate <- pars$beta_rate

    #fit the model
    if (modification == "beta") {
        fitindir <- stan(file='induced_dirichlet/dirichlet_covar_beta_group.stan', data=simdat, chains=4)
    } else {
        fitindir <- stan(file='induced_dirichlet/dirichlet_covar_alpha_group.stan', data=simdat, chains=4)
    }
    return(fitindir)
}

# run all models, parallelized

# make a cluster using half your cores
no_cores <- parallel::detectCores()/2
cl <- parallel::makeCluster(no_cores)

# export the stuff you need to run on the cluster
parallel::clusterExport(cl, c("fit_indir_model", "parlist_indir", "simdat_beta", "simdat_alpha"))
parallel::clusterEvalQ(cl, c(library(rstan), library(StanHeaders)))

reps=10
for (i in 1:reps) {
  fits_indir <- parallel::parLapply(cl, parlist_indir, function(x) {fit_indir_model(simdatlist = simdat_beta, pars=x, modification="beta")})
  saveRDS(fits_indir, file = paste0("induced_dirichlet/group_runs/beta_dat_beta_mod/run", i, ".rds"))
  rm(fits_indir)
  gc()
}

for (i in 1:reps) {
    fits_indir <- parallel::parLapply(cl, parlist_indir, function(x) {fit_indir_model(simdatlist = simdat_beta, pars=x, modification="alpha")})
  saveRDS(fits_indir, file = paste0("induced_dirichlet/group_runs/beta_dat_alpha_mod/run", i, ".rds"))
  rm(fits_indir)
  gc()
}

for (i in 1:reps) {  
    fits_indir <- parallel::parLapply(cl, parlist_indir, function(x) {fit_indir_model(simdatlist = simdat_alpha, pars=x, modification="beta")})
  saveRDS(fits_indir, file = paste0("induced_dirichlet/group_runs/alpha_dat_beta_mod/run", i, ".rds"))
  rm(fits_indir)
  gc()
}

for (i in 1:reps) {  
    fits_indir <- parallel::parLapply(cl, parlist_indir, function(x) {fit_indir_model(simdatlist = simdat_alpha, pars=x, modification="alpha")})
  saveRDS(fits_indir, file = paste0("induced_dirichlet/group_runs/alpha_dat_alpha_mod/run", i, ".rds"))
  rm(fits_indir)
  gc()
}

parallel::stopCluster(cl) #close the cluster

```

```{r readIndirModels, warning=FALSE, cache=TRUE}

# pull in parameters and info on divergences, etc from saved stanfit objects
# very slow step. consider parallelizing to the extent your ram can handle
# 
## append a label (string) to all columnnames in a dataframe (x)
label_names <- function(x, label) {
    colnames(x) <- paste0(colnames(x), "_", label)
    return(x)
}

# make_long_param <- function(params) {
#   params <- params %>%
#     pivot_longer(starts_with("alpha"), names_to = "group", values_to = "alpha_group") %>%
#       extract(group, regex="([:digit:])", into = "group")
#   return(params)
# }



calc_h <- function(pars, type) {
  if (type == "beta") {
    newpars <- pars %>%
      mutate(h.1_group.1 = c.1/(beta + beta_g.1),
             h.1_group.2 = c.1/(beta + beta_g.2),
             h.1_group.3 = c.1/(beta + beta_g.3),
             h.1_group.4 = c.1/(beta + beta_g.4),
             h.1_group.5 = c.1/(beta + beta_g.5),
             h.1_group.6 = c.1/(beta + beta_g.6),
             h.1_group.7 = c.1/(beta + beta_g.7),
             
             h.2_group.1 = c.2/(beta + beta_g.1),
             h.2_group.2 = c.2/(beta + beta_g.2),
             h.2_group.3 = c.2/(beta + beta_g.3),
             h.2_group.4 = c.2/(beta + beta_g.4),
             h.2_group.5 = c.2/(beta + beta_g.5),
             h.2_group.6 = c.2/(beta + beta_g.6),
             h.2_group.7 = c.2/(beta + beta_g.7)
      )
  }
  
  if (type == "alpha") {
    newpars <- pars %>%
      mutate(h.1_group.1 = (c.1 + alpha_g.1)/beta,
             h.1_group.2 = (c.1 + alpha_g.2)/beta,
             h.1_group.3 = (c.1 + alpha_g.3)/beta,
             h.1_group.4 = (c.1 + alpha_g.4)/beta,
             h.1_group.5 = (c.1 + alpha_g.5)/beta,
             h.1_group.6 = (c.1 + alpha_g.6)/beta,
             h.1_group.7 = (c.1 + alpha_g.7)/beta,
             
             h.2_group.1 = (c.2 + alpha_g.1)/beta,
             h.2_group.2 = (c.2 + alpha_g.2)/beta,
             h.2_group.3 = (c.2 + alpha_g.3)/beta,
             h.2_group.4 = (c.2 + alpha_g.4)/beta,
             h.2_group.5 = (c.2 + alpha_g.5)/beta,
             h.2_group.6 = (c.2 + alpha_g.6)/beta,
             h.2_group.7 = (c.2 + alpha_g.7)/beta)
  }
  return(newpars)
}

# calc_true_h <- function(truepars) {
#   newtruepars <- truepars %>%
#     mutate(h.1_group.1 = (c.1 + alpha_g.1)/beta,
#            h.1_group.2 = (c.1 + alpha_g.2)/beta,
#            h.1_group.3 = (c.1 + alpha_g.3)/beta,
#            h.1_group.4 = (c.1 + alpha_g.4)/beta,
#            h.1_group.5 = (c.1 + alpha_g.5)/beta,
#            h.1_group.6 = (c.1 + alpha_g.6)/beta,
#            h.1_group.7 = (c.1 + alpha_g.7)/beta,
#            
#            h.2_group.1 = (c.2 + alpha_g.1)/beta,
#            h.2_group.2 = (c.2 + alpha_g.2)/beta,
#            h.2_group.3 = (c.2 + alpha_g.3)/beta,
#            h.2_group.4 = (c.2 + alpha_g.4)/beta,
#            h.2_group.5 = (c.2 + alpha_g.5)/beta,
#            h.2_group.6 = (c.2 + alpha_g.6)/beta,
#            h.2_group.7 = (c.2 + alpha_g.7)/beta)
#   return(newtruepars)
# }

#bind true parameters (in list parlist) and model parameters (in list fit) even tho it will make a giant df. modelstr is string "alpha" or "beta" and describes whether model structure adds effects to intercept or slope.
bind_true_model_pars <- function(fits, parlist, modelstr) {
    # extract params from model object
    params <- lapply(fits, function(x) {data.frame(rstan::extract(x) ) } )
   
    # calculate transitions
    params <- map(params, calc_h, type=modelstr)
    parlist <- map(parlist, calc_h, type=modelstr)

    # label params as coming from the model or as true params used to
    params <- map(params, label_names, label="model")
    parlist <- map(parlist, label_names, label="true")

    # combine model and true params in a big list of dataframes - each list entry is a dataframe for a single model
    params <- map2(params, parlist, cbind)

    return(params)
}

# take a stan object and find out if there's anything egregiously wrong with it
check_model <- function(stanobj) {
    nuts <- nuts_params(stanobj)
    divergences <- dplyr::filter(nuts, Parameter=="divergent__" & Value==1) %>%
        nrow()
    rhats <- rhat(stanobj)
    bad_rhats <- sum(rhats > 1.01)
    nefrats <- neff_ratio(stanobj)
    bad_neff <- sum(nefrats < 0.1, is.nan(nefrats), na.rm=TRUE)
    diagnostics <- data.frame(divergences = divergences, bad_rhats=bad_rhats, bad_neff=bad_neff)
    return(diagnostics)
}

# run check_model on a list of models and add a column that names each row by the list name
check_list_of_models <- function(model_list) {
    map_dfr(model_list, check_model, .id=".id") %>%
        rename(modelid=.id)
}

# Extract model configurations and the most obvious problems with fit/convergence from stanfit objects. Stanfit objects are in lists in .rds files saved in path. path is a string denoting the directory where the rds files are stored. Nothing other than .rds files with lists of stanfit objects should be stored in path.
extract_pars_and_problems <- function(path, parlist, modelstr = modelstr) {
    fits <- list.files(path=path)
    reps <- length(fits)
    pars <- list()
    bad_models <- list()

    for (i in 1:reps) {
        run <- readRDS(paste0(path, fits[i])) # read in first run
        run_pars <- bind_true_model_pars(run, parlist=parlist, modelstr = modelstr) # extract parameters
        run_fit <- check_list_of_models(run) %>% # id problems
            mutate(all_bads = divergences + bad_rhats + bad_neff) %>%
            filter(all_bads > 0) %>%
            select(-all_bads)
        pars[[i]] <- run_pars
        bad_models[[i]] <- run_fit
        rm(run, run_pars, run_fit)
        gc()
    }

    return(list(pars = pars, problems = bad_models))
}

dirs <- list.dirs("induced_dirichlet/group_runs/")
extracts_indir_beta_beta <- extract_pars_and_problems(path="induced_dirichlet/group_runs/beta_dat_beta_mod/", parlist=parlist_indir, modelstr = "beta")
extracts_indir_beta_alpha <- extract_pars_and_problems(path="induced_dirichlet/group_runs/beta_dat_alpha_mod/", parlist=parlist_indir, modelstr = "alpha")
extracts_indir_alpha_beta <- extract_pars_and_problems(path="induced_dirichlet/group_runs/alpha_dat_beta_mod/", parlist=parlist_indir, modelstr = "beta")
extracts_indir_alpha_alpha <- extract_pars_and_problems(path="induced_dirichlet/group_runs/alpha_dat_alpha_mod/", parlist=parlist_indir, modelstr = "alpha")


```

Models with the effect on the slope have serious fitting issues. Lots of divergences. Models with the effect on intercept don't have fitting problems.

```{r indirModelCheck}

table_problems <- function(problemlist) {
  problems <- problemlist$problems %>%
    map_dfr(bind_rows, .id=".id") %>%
    rename(run=.id) %>%
    group_by(modelid) %>%
    summarize(bad_proportion = n()/reps, bad_count = n(), divergences_mean=mean(divergences), bad_rhats_mean=mean(bad_rhats), bad_neff_mean=mean(bad_neff))
  
  return(problems)
}

problems_bb <- table_problems(extracts_indir_beta_beta)
problems_ba <- table_problems(extracts_indir_beta_alpha)
problems_ab <- table_problems(extracts_indir_alpha_beta)
problems_aa <- table_problems(extracts_indir_alpha_alpha)
  
knitr::kable(problems_bb, caption = "Beta data, beta model")
knitr::kable(problems_ab, caption = "Alpha data, beta model")

knitr::kable(problems_ba, caption = "Beta data, alpha model")
knitr::kable(problems_aa, caption = "Alpha data, alpha model")


```

Fit on intercept models was fine no matter how data was simulated. Going forward, I'll only be using the intercept models. 

```{r}
# pull params from fits
params_ba <- extracts_indir_beta_alpha$pars %>%
  map_dfr(bind_rows, .id = ".id") %>%
  rename(run=.id) %>%
  split(.$modelid)

params_aa <- extracts_indir_alpha_alpha$pars %>%
  map_dfr(bind_rows, .id = ".id") %>%
  rename(run=.id) %>%
  split(.$modelid)


```

```{r plotIndirParams, fig.height=3, fig.width=4}

parplot <- function(modelpars) {
p1 <- ggplot(modelpars, aes(x=beta_model, color=run)) +
    geom_density()+
    theme(legend.position="none") +
    xlab("beta") +
    geom_vline(xintercept = unique(modelpars$beta_true))

p2 <- ggplot(modelpars, aes(x=c.1_model, color=run)) +
    geom_density() +
    geom_density(aes(x=c.2_model)) +
    theme(legend.position="none") +
    xlab("cutpoints") +
    geom_vline(xintercept = c(unique(modelpars$c.1_true), unique(modelpars$c.2_true)))

p3 <- ggplot(modelpars, aes(x=h.1_group.1_model, color=run)) +
    geom_density() +
    geom_density(aes(x=h.2_group.1_model)) +
    theme(legend.position="none") +
    xlab("group 1 transitions") +
    geom_vline(xintercept = c(unique(modelpars$h.1_group.1_true), unique(modelpars$h.2_group.1_true)))

cowplot::plot_grid(p1, p2, p3, labels=paste("model", modelpars$modelid_true))

}

map(params_ba[sort(base::sample(1:12, 5))], parplot) # pick 5 models at random to plot
map(params_aa[sort(base::sample(1:12, 5))], parplot)

```

# Recapture rate

```{r indirHDPI, message=FALSE, warning=FALSE, cache=TRUE}
# calculate whether true value is in HPDI
# 
HPDIlow <- function(x, prob) {
    HPDI <- rethinking::HPDI(x, prob=prob)
    return(HPDI[1])
}

HPDIhigh <- function(x, prob) {
    HPDI <- rethinking::HPDI(x, prob=prob)
    return(HPDI[2])
}

calc_HPDI <- function(params, prob) {
    params <- params %>%
        tidyr::pivot_longer(ends_with("model"), names_to = "param", values_to = "param_value") %>%
        filter(param != "lp___model") %>%
        tidyr::extract(param, into="param", regex="(.*)_model") %>% # drop model ending
        rename(modelid=modelid_true)

    # calculate bottom and top of HPDI at prob for each model run and each parameter
    low <- params %>%
        group_by(modelid, run, param) %>%
        summarise(low= HPDIlow(param_value, prob=prob))
    high <- params %>%
        group_by(modelid, run, param) %>%
        summarise(high= HPDIhigh(param_value, prob=prob))


    hdpis <- dplyr::full_join(low, high)

    # true params
    true <- params %>% dplyr::summarise_at(vars(ends_with("true")), unique)
    colnames(true) <- stringr::str_replace(colnames(true), "_true", "")
    true <- select(true, c("c.1", "c.2", "beta", starts_with("alpha"), starts_with("h."))) %>%
        tidyr::pivot_longer(cols=c("c.1", "c.2", "beta", starts_with("alpha"), starts_with("h.")), names_to = "param", values_to="true")

    # combine true and hdpis for comparison
    compframe <- full_join(hdpis, true)


    # true param in interval?
    tf <- compframe %>% mutate(inint = true > low & true < high)
    return(tf)
}

# For each parameter (5) in each model (27) in each run (30), is a given parameter in the 50% or 90% HPDI? modelpars is a list of dataframes. Each dataframe in the list contains parameters from 30 runs of 1 model along with the true parameters used to simulate the datasets.
# output is a list of 2 dataframes 4050 rows each (5x27x30) - with each parameter estimated by the model and an inint column with TRUE if it falls in the HPDI interval and FALSE if not.
which_params_recaptured <- function(modelpars) {
    in50 <- map(modelpars, calc_HPDI, prob=0.5)

    #in75 <- map(params_indir, calc_HPDI, prob=0.75)

    in90 <- map(modelpars, calc_HPDI, prob=.90)

    # recaptured parameters (refactor this later so it's all in one df
    perform50 <- map_dfr(in50, bind_rows)
    perform90 <- map_dfr(in90, bind_rows)

    return(list(fifty = perform50, ninety=perform90))
}


recaptured_ba <- which_params_recaptured(params_ba)
recaptured_aa <- which_params_recaptured(params_aa)

calc_prop_recaptured_overall <- function(inint, truepars) {

# trues <- inint$fifty %>%
#   ungroup() %>%
#   select(modelid, param, true) %>%
#   distinct()
# 
# nas <- which(is.na(inint$fifty$inint))
# inint$fifty[nas,]

    # proportion of parameters recaptured
    prop_recaptured50 <- inint$fifty %>%
      filter(param != "sigma_group") %>%
        group_by(modelid, run)  %>%
        summarise(captured = sum(inint, na.rm = TRUE))  %>%
        summarise(mean_captured = mean(captured), sd_captured=sd(captured)) %>%
        full_join(truepars) %>%
        arrange(beta, desc(mean_captured))


    prop_recaptured90 <- inint$ninety %>%
      filter(param != "sigma_group") %>%
        group_by(modelid, run) %>%
        summarise(captured = sum(inint, na.rm=TRUE)) %>%
        summarise(mean_captured = mean(captured), sd_captured=sd(captured)) %>%
        full_join(truepars) %>%
        arrange(beta, desc(mean_captured))

    return(list(fifty = prop_recaptured50, ninety = prop_recaptured90))
}

prop_recaptured_ba <- calc_prop_recaptured_overall(recaptured_ba, truepars=parframe_indir)
prop_recaptured_aa <- calc_prop_recaptured_overall(recaptured_aa, truepars=parframe_indir)

calc_recaptured_by_param <- function(inint, truepars) {
    perform50_summary <- inint$fifty %>%
      filter(param != "sigma_group") %>%
        group_by(modelid, param) %>%
        summarise(prop_inint = mean(inint)) %>%
        left_join(truepars)

    perform90_summary <- inint$ninety %>%
      filter(param != "sigma_group") %>%
        group_by(modelid, param) %>%
        summarise(prop_inint = mean(inint)) %>%
        left_join(truepars)

    return(list(fifty=perform50_summary, ninety=perform90_summary))
}

prop_recaptured_by_param_ba <- calc_recaptured_by_param(recaptured_ba, parframe_indir) 
prop_recaptured_by_param_aa <- calc_recaptured_by_param(recaptured_aa, parframe_indir) 
```




```{r plotIndirRecaptureoverall, caption="Number of parameters recaptured by each model, averaged across model runs", fig.width=7, fig.height=4}
ggplot(prop_recaptured_ba$fifty, aes(x=as.factor(anchor), y=as.factor(beta_rate), fill=mean_captured)) + 
  geom_tile(colour="white") +
  geom_text(aes(label=modelid)) +
  facet_wrap("transition", scales="free") +
  ggtitle("Proportion of parameters recaptured in 50% HPDI", subtitle = "faceted by transition speed in simulated dataset \n labeled with modelid") +
  scale_fill_viridis_c() +
  ylab("beta_rate")
# ggplot(prop_recaptured_aa$fifty, aes(x=as.factor(anchor), y=as.factor(beta_rate), fill=mean_captured)) + 
#   geom_tile(colour="white") +
#   geom_text(aes(label=modelid)) +
#   facet_wrap("transition", scales="free") +
#   ggtitle("Proportion of parameters recaptured in 50% HPDI", subtitle = "faceted by transition speed in simulated dataset \n labeled with modelid") +
#   scale_fill_viridis_c(limits=c(0,10)) +
#   ylab("beta_rate")

ggplot(prop_recaptured_ba$ninety, aes(x=as.factor(anchor), y=as.factor(beta_rate), fill=mean_captured)) + 
  geom_tile(colour="white") +
  geom_text(aes(label=modelid)) +
  facet_wrap("transition", scales="free") +
  ggtitle("Proportion of parameters recaptured in 90% HPDI",subtitle = "faceted by transition speed in simulated dataset \n labeled with modelid") +
  scale_fill_viridis_c(limits=c(0,17)) +
  ylab("beta_rate")

# ggplot(prop_recaptured_aa$ninety, aes(x=as.factor(anchor), y=as.factor(beta_rate), fill=mean_captured)) + 
#   geom_tile(colour="white") +
#   geom_text(aes(label=modelid)) +
#   facet_wrap("transition", scales="free") +
#   ggtitle("Proportion of parameters recaptured in 90% HPDI",subtitle = "faceted by transition speed in simulated dataset \n labeled with modelid") +
#   scale_fill_viridis_c(limits=c(0,10)) +
#   ylab("beta_rate")
```
Model is better able to return parameters when data generating process in on intercept.

But what about the relationship between the variables - the transition points?

```{r plotIndirRecapture}

ggplot(prop_recaptured_by_param_ba$fifty, aes(x=param, y=as.factor(modelid), fill=prop_inint)) + 
  geom_tile(color="white") +
  scale_fill_viridis_c()  +
  geom_text(aes(label=paste(anchor, beta_rate, sep="\n")), size=2.5) +
  facet_wrap("transition", scales="free") +
  ggtitle("Parameters recaptured in 50% HPDI", subtitle = "faceted by transition speed (beta) in simulated dataset \n with anchor and beta rate labels") +
  theme(legend.position = "top") +
  ylab("model id")  +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

ggplot(prop_recaptured_by_param_ba$ninety, aes(x=param, y=as.factor(modelid), fill=prop_inint)) + 
  geom_tile(color="white") +
  scale_fill_viridis_c()  +
  geom_text(aes(label=paste(anchor, beta_rate, sep="\n")), size=2.5) +
  facet_wrap("transition", scales="free") +
  ggtitle("Parameters recaptured in 90% HPDI", subtitle = "faceted by transition speed (beta) in simulated dataset\n with anchor and beta rate labels") +
  theme(legend.position = "top") +
  ylab("model id") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```

Only very small deviations from h (or alpha) are recaptured.
