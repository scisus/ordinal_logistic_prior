---
title: "Choosing priors in an ordinal logistic model"
subtitle: "with covariate and group effects"
author: "Susannah Tysor"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  html_document:
    theme: flatly
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message=FALSE)
```

```{r}
library(dplyr)
library(rstan)
library(tidyr)
library(purrr)
library(cowplot)
library(bayesplot)
library(ggplot2)

rstan_options(auto_write=TRUE)
```

```{r depends}
source('prior_analysis_groups_functions.R')
```

```{r global}
# each version of a model should be run how many times?
reps <- 5
```

# Background

[Previously](https://scisus.github.io/ordinal_logistic_prior/prior_analysis.html), I considered whether a gamma or an induced dirichlet prior on cut points in an ordinal logistic model worked better for the kind of data I'm considering. Neither was very promising, but because of problems with convergence, etc. in models with the gamma prior, I'm testing group effects with the induced dirichlet.

In this model, the state of flowering depends on how many forcing units have accumulated. 

There are three possible states: 1-not yet flowering, 2- flowering, 3- done flowering. 

$x$ is accumulated forcing: when flowering was observed, how much warmth had the population/tree been exposed to since January 1? $x$ is always positive (and always increases monotonically through time - though time is abstracted out of this model).

$\beta$ describes how fast the transition occurs - small $\beta$s make the transition from between states occur over a wider range of $x$'s. (Translated from forcing units to days, this answers a question like "does the population transition between states over 1 day, 3 days, a week?" We work in forcing unit space because the trees respond to temperature, not time - and no spring heats up exactly the same so dates are kind of useless for prediction.) $\beta$ is always positive.

In [the case without groups](https://scisus.github.io/ordinal_logistic_prior/prior_analysis.html), an ordinal logistic model with an induced dirichlet prior on cutpoints can struggle to recapture $\beta$ and cut points $c$, but it is pretty good at capturing the relationship between $\beta$ and $c$: $h = \frac{c}{\beta}$, which is the point at which half the trees in a population have transitioned or the point at which an individual tree is 50% likely to have transitioned. 

In the previous analysis, no groups were considered. In reality, I want to know if things like site, provenance, and clone affect $h$.

## Goals
1 - Generate data with an ordinal logistic model 
   - With linear model structure $\beta x + \alpha$
2 - Fit model with Stan
3 - Determine whether $\beta$, $\beta_{site_i}$, \alpha$, $c$, or $h$ can be returned.


I'll simulate data for 10 groups with `N=100` observations for each group. Each group's $h$ are shifted by $h_mod$, which is drawn from a $\mathcal{N}(0,\sigma)$. Data is simulated for two transition speeds, $\beta=1$ and $\beta=2$.

First we'll do this with null effects ($\sigma=0$), then with a relatively large range of effects (\sigma = 1).

# Can null group effects be detected?

Simulate data from an ordinal logistic model where there are 10 groups, but none of the groups have an effect.

The Stan model for data simulation is:

```{r}
writeLines(readLines("simulate/covar_group_alpha_sim.stan"))
```
Plots of simulated data for each group should look roughly identical.

```{r nullGroupSimulation, fig.height=11, fig.width=5, fig.cap="Simulated data in each group. Since groups have no effect in test, all groups should look more or less identical."}

G = 10
h_mod0 <- rep(0, G)

simulation_input <- set_simulation_parameters(G=G, hmod=h_mod0)


simdat_alpha <- purrr::map(.x=simulation_input$inputlist, .f=simulate_data)

plot_simulated_data(simdat_alpha, simulation_input$inputlist)
```

## Estimate parameters

Next we'll check whether models can accurately recapture parameters. Based on findings from the no-group investigation, we'll create models with an exponential prior on $\beta$ with a rate of 1, 2, or 3 and the anchor for the dirichlet inducing the priors on cutpoints at 10 and 20. 



```{r params}

beta_rate <- c(1:3) # rate parameters for exponential prior on beta
anchor <- c(10, 20) # different anchor parameters for induced dirichlet prior

# make a nice dataframe with all combinations params used to simulate data and model params used to try to recover those params
# 

parframe_indir <- simulation_input$pars %>% 
  dplyr::select(transition, beta, c.1, c.2, group, alpha_group) %>%
  tidyr::pivot_wider(names_from = group, names_prefix = "alpha_g.", values_from = alpha_group) %>% # put in wide format
  merge(y= beta_rate) %>%
    rename(beta_rate=y) %>%
    merge(y=anchor) %>%
    rename(anchor=y)
parframe_indir$modelid <- 1:nrow(parframe_indir) #label the models

# make a wide format with beta_group cols and merge with parframe_indir


parlist_indir <- make_parframe_list(parframe_indir)

# model parameterizations
parameterizations_indir <- select(parframe_indir, anchor, beta_rate) %>%
  distinct() %>%
  arrange(anchor, beta_rate)

table_indir <- parframe_indir %>% 
  select(transition, modelid, anchor, beta_rate) %>%
  group_by(transition, modelid, anchor, beta_rate) %>%
  tidyr::pivot_wider(names_from=transition, values_from=modelid) %>%
  tidyr::unite("modelids", c("fast", "medium"), sep=", ", remove=TRUE) %>%
  right_join(parameterizations_indir) %>%
  select(modelids, anchor, beta_rate)

```

```{r paramsTableIndir}
knitr::kable(table_indir, caption="Prior parameters. modelids are unique identifiers for each combination of prior parameters and simulated datasets, in order from fast transition to medium transition dataset")
```
I'm going to do `r reps` runs of each of the 12 models in Stan.

```{r indirRecapture, include=FALSE}


# # run all models, parallelized
# 
# # make a cluster using half your cores
# no_cores <- parallel::detectCores()/2
# cl <- parallel::makeCluster(no_cores)
# 
# # export the stuff you need to run on the cluster
# parallel::clusterExport(cl, c("fit_indir_model", "parlist_indir", "simdat_alpha"))
# parallel::clusterEvalQ(cl, c(library(rstan), library(StanHeaders)))
# 
# for (i in 1:reps) {
#     fits_indir <- parallel::parLapply(cl, parlist_indir, function(x) {fit_indir_model(simdatlist = simdat_alpha, pars=x})
#   saveRDS(fits_indir, file = paste0("induced_dirichlet/group_runs/alpha_dat_alpha_mod_nullgroup/run", i, ".rds"))
#   rm(fits_indir)
#   gc()
# }
# 
# parallel::stopCluster(cl) #close the cluster

```

The Stan program fitting these models is

```{r}
writeLines(readLines("induced_dirichlet/dirichlet_covar_alpha_group.stan"))
```


Models have very serious problems (divergences, large rhats, low effective sample sizes, etc.) if the variance on group effects has a default (uniform) or $\mathrm{Exponential}(1)$ distribution. 

```{r readIndirModels, warning=FALSE, cache=TRUE}

# pull in parameters and info on divergences, etc from saved stanfit objects
# very slow step. consider parallelizing to the extent your ram can handle
# 


dirs <- list.dirs("induced_dirichlet/group_runs/")

extracts_indir_alpha_alpha_ng <- extract_pars_and_problems(
  path="induced_dirichlet/group_runs/alpha_dat_alpha_mod_nullgroup/", 
  parlist=parlist_indir)


```


```{r indirModelCheck}



problems_aang <- table_problems(extracts_indir_alpha_alpha_ng)
  
knitr::kable(problems_aang, caption = "Alpha data, alpha model")


```

A few of the models have rhats that are too large in some runs. The problems occur with data where the transition speed is moderate ($\beta=1$), the prior on $\beta$ is loose or tight, and the anchor is large.

It is a problem that the model can't fit $\alpha_{sigma}=0$

```{r}
# pull params from fits
# params_ba <- extracts_indir_beta_alpha$pars %>%
#   map_dfr(bind_rows, .id = ".id") %>%
#   rename(run=.id) %>%
#   split(.$modelid)

params_aang <- extracts_indir_alpha_alpha_ng$pars %>%
  map_dfr(bind_rows, .id = ".id") %>%
  rename(run=.id) #%>%
  #split(.$modelid)


```

$\beta$ and $c$ are always underestimated. $h$ is estimated ok when the true transition speed is rapid, and poorly when it's slower. Effects ($\alpha_g$) are generally centered on 0 and are quite wide like the prior.
```{r plotIndirParams, cache=TRUE}


#map(params_ba[sort(base::sample(1:12, 5))], parplot) # pick 5 models at random to plot
#map(params_aang[sort(base::sample(1:12, 5))], parplot)

plot_densities_anchorxbeta(params_aang, "beta_model", "beta_true")
plot_densities_anchorxbeta(params_aang, "c.1_model", "c.1_true")
plot_densities_anchorxbeta(params_aang, "c.2_model", "c.2_true")
plot_densities_anchorxbeta(params_aang, "h.1_group.1_model", "h.1_group.1_true")
plot_densities_anchorxbeta(params_aang, "h.1_group.5_model", "h.1_group.5_true")
plot_densities_anchorxbeta(params_aang, "h.1_group.10_model", "h.1_group.10_true")
plot_densities_anchorxbeta(params_aang, "alpha_g.1_model", "alpha_g.1_true")
plot_densities_anchorxbeta(params_aang, "alpha_g.2_model", "alpha_g.2_true")
plot_densities_anchorxbeta(params_aang, "alpha_g.3_model", "alpha_g.3_true")
plot_densities_anchorxbeta(params_aang, "alpha_g.4_model", "alpha_g.4_true")
plot_densities_anchorxbeta(params_aang, "alpha_g.9_model", "alpha_g.9_true")

```

## Recapture rate

```{r indirHDPI, message=FALSE, warning=FALSE, cache=TRUE}

recaptured_aang <- params_aang %>%
  split(.$modelid) %>%
  which_params_recaptured()

prop_recaptured_aang <- calc_prop_recaptured_overall(recaptured_aang, truepars=parframe_indir)

prop_recaptured_by_param_aang <- calc_recaptured_by_param(recaptured_aang, parframe_indir)  %>%
  purrr::map_dfr(bind_rows, .id=".id")
```



```{r plotIndirRecaptureoverall, caption="Number of parameters recaptured by each model, averaged across model runs"}

ggplot(prop_recaptured_aang$fifty, aes(x=transition, y=mean_captured, fill=transition)) +
  geom_bar(stat="identity") +
  geom_bar(prop_recaptured_aang$ninety, stat="identity", alpha=0.5, mapping = aes(x=transition, y=mean_captured, fill=transition), inherit.aes = FALSE) +
  facet_grid(anchor ~ beta_rate) +
  ylim(c(0,34)) +
  ggtitle("Mean # of parameters captured in 50% and 90% HPDI") +
  geom_hline(yintercept=34)



```
Models for data with fast transitions capture more true parameters in their 50% HPDIs than data with medium transitions, but medium transitions capture more true parameters in their 90% HPDIs. Medium speed transitions, however, tend to have fitting issues unless the prior on $\beta$ is just right. No model captures all parameters. There isn't a dramatic difference between priors, though a tight $\beta$ prior and small anchor depresses the number of parameters estimated correctly for fast transition datasets.

###Which parameters are easier or harder to recapture?

```{r plotIndirRecapture, fig.height=11}

ggplot(dplyr::filter(prop_recaptured_by_param_aang, .id=="fifty"), mapping = aes(x=param, y=prop_inint, fill=transition)) +
  geom_bar(stat="identity", position="dodge") +
  geom_bar(dplyr::filter(prop_recaptured_by_param_aang, .id=="ninety"), stat="identity", position="dodge", alpha=0.5, mapping = aes(x=param, y=prop_inint, fill=transition), inherit.aes = FALSE) +
  facet_grid(anchor  ~ transition + beta_rate) +
  ylim(c(0,1)) +
  ggtitle("Mean # of parameters captured in 50% and 90% HPDI", subtitle = "facet columns are transition and beta rate, rows are anchors") +
  theme(legend.position = "top") +
  coord_flip()


```

As one might have expected from the results in a model with less structure in the data, $\beta$ and $c$ aren't returned well. They are never returned for fast transition datasets and they are only returned in the 90% HPDI for medium transition datasets.)

The transition points and effects $\alpha$ fare better. The true value, `0`, is almost always in the 90% HPDI and, for many groups, is in the 50% HPDI. But the 90% HPDI is relatively large - there's a fair amount of uncertainty there.


# Can group effects in a range of sizes be detected?

While I am hoping group effects will largely be 0 or very close to it, I need to know if they are not. So, now that we know the model can detect null effects, I need to know what size effects it can detect. To that end, I will simulate data from an ordinal logistic model as above, but this time with effects on the intercept $\alpha$ distributed $\mathcal{N}(0,1)$

```{r GroupSimulation, fig.height=10, fig.width=5, fig.cap="Simulated data in each group."}


# h_mod <- rnorm(n=G) # how much being in each group modifies half transitions
h_mod <- c(  -0.99,  0.27,  0.57,  0.01,  0.67,  1.63, -2.05,  0.46, -0.03,  0.28)

simulation_inputg <- set_simulation_parameters(G=G, hmod=h_mod, noeffect = FALSE)

simdat_alphag <- map(simulation_inputg$inputlist, simulate_data)

plot_simulated_data(simdat_alphag, simulation_inputg$inputlist)
```

Since groups have different effects, they should look different in this plot.

## Estimate parameters

Does the model accurately recapture parameters? I'll use the same priors as above, and the same Stan model.

```{r paramsg}

beta_rate <- c(1:3) # rate parameters for exponential prior on beta
anchor <- c(10, 20) # different anchor parameters for induced dirichlet prior

# make a nice dataframe with all combinations params used to simulate data and model params used to try to recover those params
# 

parframe_indirg <- simulation_inputg$pars %>% 
  dplyr::select(transition, beta, c.1, c.2, group, alpha_group) %>%
  tidyr::pivot_wider(names_from = group, names_prefix = "alpha_g.", values_from = alpha_group) %>% # put in wide format
  merge(y= beta_rate) %>%
    rename(beta_rate=y) %>%
    merge(y=anchor) %>%
    rename(anchor=y)
parframe_indirg$modelid <- 1:nrow(parframe_indir) #label the models

# make a wide format with beta_group cols and merge with parframe_indir


parlist_indirg <- make_parframe_list(parframe_indirg)

# model parameterizations
parameterizations_indirg <- select(parframe_indirg, anchor, beta_rate) %>%
  distinct() %>%
  arrange(anchor, beta_rate)

table_indirg <- parframe_indirg %>% 
  select(transition, modelid, anchor, beta_rate) %>%
  group_by(transition, modelid, anchor, beta_rate) %>%
  tidyr::pivot_wider(names_from=transition, values_from=modelid) %>%
  tidyr::unite("modelids", c("fast", "medium"), sep=", ", remove=TRUE) %>%
  right_join(parameterizations_indirg) %>%
  select(modelids, anchor, beta_rate)

```

```{r indirRecaptureg, include=FALSE}

# # run all models, parallelized
# 
# # make a cluster using half your cores
# cl <- parallel::makeCluster(10)
# 
# # export the stuff you need to run on the cluster
# parallel::clusterExport(cl, c("fit_indir_model", "parlist_indir", "simdat_alphag"))
# parallel::clusterEvalQ(cl, c(library(rstan), library(StanHeaders)))
# 
# for (i in 1:reps) {
#   fits_indir <- parallel::parLapply(cl, parlist_indir, function(x) {fit_indir_model(simdatlist = simdat_alphag, pars=x)})
#   saveRDS(fits_indir, file = paste0("induced_dirichlet/group_runs/alpha_dat_alpha_mod_group/run", i, ".rds"))
#   rm(fits_indir)
#   gc()
# }
# 
# parallel::stopCluster(cl) #close the cluster

```

```{r readIndirModelsg, warning=FALSE, cache=TRUE}

# pull in parameters and info on divergences, etc from saved stanfit objects

dirs <- list.dirs("induced_dirichlet/group_runs/")

extracts_indir_alpha_alpha_g <- extract_pars_and_problems(path="induced_dirichlet/group_runs/alpha_dat_alpha_mod_group/", parlist=parlist_indirg)


```


Again, models for data with medium speed transitions struggled - generally rhat too large. Problems don't occur with models for fast transition speed data. When models have an anchor of 10 and relatively constrained $\beta$ priors (rate 2 or 3) or an anchor of 20 and a tight $\beta$ prior (3), models for medium transition speed don't run into convergence, etc. problems.

```{r indirModelCheckg}


problems_aag <- table_problems(extracts_indir_alpha_alpha_g)
  

knitr::kable(problems_aag, caption = "Alpha data, alpha model")

```


```{r}
# pull params from fits
# params_ba <- extracts_indir_beta_alpha$pars %>%
#   map_dfr(bind_rows, .id = ".id") %>%
#   rename(run=.id) %>%
#   split(.$modelid)

params_aag <- extracts_indir_alpha_alpha_g$pars %>%
  map_dfr(bind_rows, .id = ".id") %>%
  rename(run=.id) #%>%
  #split(.$modelid)


```

$\beta$ and $c$ are always underestimated, and quite dramatically when underlying transition speed is medium. $h$ is underestimated as well, though not quite as badly.
```{r plotIndirParamsg}


#map(params_ba[sort(base::sample(1:12, 5))], parplot) # pick 5 models at random to plot
#map(params_aang[sort(base::sample(1:12, 5))], parplot)

plot_densities_anchorxbeta(params_aag, "beta_model", "beta_true")
plot_densities_anchorxbeta(params_aag, "c.1_model", "c.1_true")
plot_densities_anchorxbeta(params_aag, "c.2_model", "c.2_true")
plot_densities_anchorxbeta(params_aag, "h.1_group.1_model", "h.1_group.1_true")
plot_densities_anchorxbeta(params_aag, "h.1_group.5_model", "h.1_group.5_true")
plot_densities_anchorxbeta(params_aag, "h.1_group.10_model", "h.1_group.10_true")
plot_densities_anchorxbeta(params_aag, "alpha_g.1_model", "alpha_g.1_true")
plot_densities_anchorxbeta(params_aag, "alpha_g.2_model", "alpha_g.2_true")
plot_densities_anchorxbeta(params_aag, "alpha_g.3_model", "alpha_g.3_true")
plot_densities_anchorxbeta(params_aag, "alpha_g.4_model", "alpha_g.4_true")
plot_densities_anchorxbeta(params_aag, "alpha_g.9_model", "alpha_g.9_true")

```

## Recapture rate

```{r indirHDPIg, message=FALSE, warning=FALSE, cache=TRUE}

recaptured_aag <- params_aag %>%
  split(.$modelid) %>%
  which_params_recaptured()

prop_recaptured_aag <- calc_prop_recaptured_overall(recaptured_aag, truepars=parframe_indirg)

prop_recaptured_by_param_aag <- calc_recaptured_by_param(recaptured_aag, parframe_indirg)  %>%
  purrr::map_dfr(bind_rows, .id=".id")
```



```{r plotIndirRecaptureoverallg, caption="Number of parameters recaptured by each model, averaged across model runs"}

ggplot(prop_recaptured_aag$fifty, aes(x=transition, y=mean_captured, fill=transition)) +
  geom_bar(stat="identity") +
  geom_bar(prop_recaptured_aag$ninety, stat="identity", alpha=0.5, mapping = aes(x=transition, y=mean_captured, fill=transition), inherit.aes = FALSE) +
  facet_grid(anchor ~ beta_rate) +
  ylim(c(0,34)) +
  ggtitle("Mean # of parameters captured in 50% and 90% HPDI") +
  geom_hline(yintercept=34)



```
Most parameters are not captured in the 50% HPDI. Parameters for medium transition speed data are easier to recapture.

```{r plotIndirRecaptureg, fig.height=11}

ggplot(dplyr::filter(prop_recaptured_by_param_aag, .id=="fifty"), mapping = aes(x=param, y=prop_inint, fill=transition)) +
  geom_bar(stat="identity", position="dodge") +
  geom_bar(dplyr::filter(prop_recaptured_by_param_aag, .id=="ninety"), stat="identity", position="dodge", alpha=0.5, mapping = aes(x=param, y=prop_inint, fill=transition), inherit.aes = FALSE) +
  facet_grid(anchor  ~ transition + beta_rate) +
  ylim(c(0,1)) +
  ggtitle("Mean # of parameters captured in 50% and 90% HPDI", subtitle = "facet columns are transition and beta rate, rows are anchors") +
  theme(legend.position = "top") +
  coord_flip()


```

As one might have expected from the results in a model with less structure in the data, $\beta$ and $c$ aren't returned ever in the 50% HPDI. For medium transition speed data, they are returned in the 90% HPDI, but never for fast transition speed data.

The transition points and effects $\alpha$ fare better. The true value for $h$ and $\alpha$ are generally returned in the 90% HPDI but very rarely in the 50%.

# tl;dr

Null groups are recaptured, but probably with too much uncertainty to be useful in my situation. Group effects and transitions are not recaptured very well -$h$ is consistently underestimated. And the actual params $\beta$ and $c$ are not trustworthy.

